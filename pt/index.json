[{"authors":null,"categories":null,"content":"Sou geógrafo e atuo como cientista de dados geográficos para enfrentar os desafios ambientais, articulando o poder dos dados de observação da Terra, estatísticas espaciais e desenvolvimento de software.\nComo eu cheguei aqui? Enquanto trabalhava na avaliação do estado de conservação da flora brasileira, percebi que, naquela época, havia muito o que fazer e poucos recursos humanos. Por isso, resolvi me especializar em estatística espacial em busca de ferramentas que pudessem auxiliar na otimização da avaliação. Concluí com uma abordagem estatística para estimar a exposição das espécies de floras ao fogo, um dos proxys mais conhecidos de degradação ambiental. Caso tenha interesse, o Trabalho final de Conclusão de Curso está disponível aqui.\nEm meu projeto de mestrado trabalhei com priorização espacial para compatibilizar a conservação e a exploração mineral em uma Reserva Ambiental na Floresta Amazônica. Você pode ler mais sobre essa experiência aqui ou acessar o livro com todos os detalhes do projeto.\nNos últimos anos tenho trabalhado com um grupo internacional de pesquisadores em projetos desenvolvendo algoritmos e identificando áreas prioritárias para restauração de paisagens. Esse trabalho teve como resultado a pulicação de várias publicações científicas.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":-62135596800,"objectID":"26f7143ab6a67e9866e2f9d6bd18a253","permalink":"/pt/author/felipe-sodre-mendes-barros/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/pt/author/felipe-sodre-mendes-barros/","section":"authors","summary":"Sou geógrafo e atuo como cientista de dados geográficos para enfrentar os desafios ambientais, articulando o poder dos dados de observação da Terra, estatísticas espaciais e desenvolvimento de software.\nComo eu cheguei aqui?","tags":null,"title":"Felipe Sodré Mendes Barros","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"pt","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/pt/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/pt/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/pt/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/pt/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/pt/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/pt/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Felipe"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/pt/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/pt/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":[],"content":"Há algum tempo comecei a perceber um \u0026ldquo;comportamento estranho\u0026rdquo; (ainda que tenha colocado o termo bug no título, acho que não é o caso. Foi para atrir mais atenção, mesmo:) relacionado aos dados de data e hora num sistema que estava desenvolvendo. Minha reação inicial, praticamente um instinto de sobrevivência, foi simplesmente resolver a situação contornando o problema. Mas chegou um momento que precisei entender a origem do mesmo. Mais uma vez tive que fazer um exercício de seguir/isolar o problema que me assombrava (veja outros artigos que produzi sobre bugs/comportamentos estranhos) para tentar compreender o motivo da sua existência. Esse processo tomou-me alguns dias e, claro, proporcionou alguns aprendizados.\nAinda que agora, tendo resolvido e entendido as causas e origens desse comportamento, tudo parece óbvio, decidi compartilhar um pouco deste processo, pois nessa busca por soluções não encontrei nada que me ajudasse de forma objetiva.\nCriei um ambiente para reproduzir esses \u0026ldquo;comportamentos estranhos\u0026rdquo; (há uma seção sobre como preparar um ambiente para poder reproduzir esses códigos) e deixarei os trechos de códigos usados para vocês poderem reproduzir os passos dados. Irei trabalhar em todos os exemplos com um mesmo objeto de data e hora (instância datetime) mudando apenas o uso de fuso horário, para torná-los conscientes (aware) ou não (naive, ingênuo) (leia um pouco sobre isso aqui). Na seção final, \u0026ldquo;resumo\u0026rdquo;, deixo os principais aprendizados deste processo.\nContextualizando o sistema Antes de tudo, lhes resumo a parte que importa do sistema:\nO mesmo estava em um servidor com fuso horário UTC, e nele eu manipulava um dado de data e hora, usando o módulo python datetime, com time zone consciente (aware), transformando-os ao time zone de Brasília (-0300). Esse dado era, então, persistido no banco de dados PostgreSQL, que estava em outro servidor, também com fuso horário UTC. Os dados eram persistidos em duas colunas diferentes: uma coluna DateTime com time zone consciente e numa coluna de texto onde, além da data e hora em formato iso, uma observação textual era adicionada (que não vem ao caso, agora). Mas é importante saber que tínhamos o mesmo dado de data e hora persistido como tal e como texto.\nUm detalhe não menos importante é o fato de eu estar usando o módulo pytz para definir o fuso America/Sao_Paulo, e o SQLAlchemy, para fazer a conexão com o banco de dados, commit e etc. Pensando em facilitar a minha vida, estive usando o DBeaver, uma interface gráfica para gestão de banco de dados. Ou seja, usava o DBeaver para conectar ao banco de dados e observar o que estava sendo persistido sem precisar fazê-lo pelo psql.\nReproduzindo comportamentos estranhos Basicamente criei uma instância datetime ingênua (naive) em relação ao time zone e outra com time zone declarado ( consciente, aware). Criei uma instância da tabela persistindo cada dado nas suas respectivas colunas (consciente na coluna consciente e ingênuo na coluna ingênua) (veja aqui sobre a criação do ambiente para reproduzir esses códigos).\nimport pytz from datetime import datetime from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker engine = create_engine( f\u0026quot;postgresql+psycopg2://postgres:password@localhost:5432/postgres\u0026quot;) Session = sessionmaker(bind=engine) session = Session() BR_TIME_ZONE = pytz.timezone(\u0026quot;America/Sao_Paulo\u0026quot;) naive = datetime(2022, 5, 27, 12, 30, 0, 0) aware = naive.replace(tzinfo=BR_TIME_ZONE) record = DateTimeTable( date_time_tz_aware=aware, isoformat_tz_aware=f\u0026quot;{aware.isoformat()}\u0026quot;, date_time_naive=naive, isoformat_naive=f\u0026quot;{naive.isoformat()}\u0026quot; ) session.add(record) session.commit() session.close()  Ao fazer o commit e consultar a base de dados, começa o terror e pânico:\nUsando o DBeaver para acessar o registro criado (seja pela interface gráfica como pela query da GUI), observei que:\n O valor persistido na coluna consciente foi alterado em seis minutos (acrescidos). Deveria ser 12:30 e passou a ser 12:36, ao passo que a informação de time zone é apresentada de forma correta: -0300; O dado da coluna iso_format_tz_aware possui a informação sem qualquer alteração. Ao passo que a time zone informada não é a esperada (-0300), mas -03:06. Lembrem-se que o time zone da coluna date_time_aware é informado apenas -0300; Os dados persistidos nos campos time zone ingênuos não apresentaram qualquer alteração.     id date_time_tz_aware iso_format_tz_aware date_time_naive isofomat_naive     1 2022-05-27 12:36:00.000 -0300 2022-05-27T12:30:00-03:06 2022-05-27 12:30:00.000 2022-05-27T12:30:00    Contudo, ao acessar esses dados usando o SQLAlchemy, a confusão aumenta:\n   id date_time_tz_aware iso_format_tz_aware date_time_naive isofomat_naive      1 2022-05-27 15:36:00+00:00 2022-05-27T12:30:00-03:06 2022-05-27 12:30:00 2022-05-27T12:30:00     Reparem que agora temos:\n Na coluna date_time_tz_aware, o objeto tem três horas e seis minutos acrescidos e o time zone informado como UTC (+00:00). Os dados das colunas iso_format, date_time_naive e isoformat_naive apresentam os dados assim como estão no banco de dados.  Comportamentos estranhos a serem resolvidos:\n O time zone deveria ser de -0300. De onde veio os seis minutos a mais? Afinal, o dado é persistido no banco de dados em UTC (como retornado pelo SQLAlchemy) ou no fuso horário informado no objeto datetime (como retornado pelo DBeaver)?  Resolvendo problema de definição de time zone Ao apresentar esses problemas aos amigos que tenho como referência na área, um deles, o @georgersilva, me alertou que a forma como eu estava definido o time zone estava equivocado. A única direção dada por ele foi essa pergunta no Stack Overflow.\nUm comentário me chamou a atenção:\n @MichaelWaterfall: pytz.timezone() may correspond to several tzinfo objects (same place, different UTC offsets, time zone abbreviations). tz.localize(d) tries to find the correct tzinfo for the given d local time (some local time is ambiguous or doesn\u0026rsquo;t exist). replace() just sets whatever (random) info pytz time zone provides by default without regard for the given date (LMT in recent versions). tz.normalize() may adjust the time if d is a non-existent local time e.g., the time during DST transition in Spring (northern hemisphere) otherwise it does nothing in this case.\n Em tradução livre:\n pytz.timezone() pode corresponder a objetos com diferentes tzinfo (mesmo local, diferentes offset em relação ao UTC). tz.localize(d) tenta encontrar o tzinfo correto para um dada hora local (algumas horas locais são ambíguas ou inexistentes). replace() apenas define qualquer informação de time zone por padrão sem se preocupar com a data. tz.normalize() deve ajustar a informação de tempo se o objeto d não possuir informação de hora local.\n Como estou usando o pytz para definir um objeto de data com fuso horário, o replace não seria a forma correta, mas sim, o método localize da própria instância pytz.timezone.\nVamos testar, então:\nBR_TIME_ZONE = pytz.timezone(\u0026quot;America/Sao_Paulo\u0026quot;) naive = datetime(2022, 5, 27, 12, 30, 0, 0) naive.replace(tzinfo=BR_TIME_ZONE) # datetime.datetime(2022, 5, 27, 12, 30, tzinfo=\u0026lt;DstTzInfo 'America/Sao_Paulo' LMT-1 day, 20:54:00 STD\u0026gt;) BR_TIME_ZONE.localize(naive) # datetime.datetime(2022, 5, 27, 12, 30, tzinfo=\u0026lt;DstTzInfo 'America/Sao_Paulo' -03-1 day, 21:00:00 STD\u0026gt;)  Reparem a diferença que isso fez no parâmetro tzinfo da instância: há uma diferença de seis minutos no objeto ao qual usei o método replace(). Ao usar o localize(), a informação de fuso horário \u0026ldquo;-03\u0026rdquo; aparece.\nFiz mais um teste para entender se o problema é o método replace ou a forma como o pytz define o time zone:\ndatetime(2022, 5, 27, 12, 30, 0, 0, tzinfo=BR_TIME_ZONE) # datetime.datetime(2022, 5, 27, 12, 30, tzinfo=\u0026lt;DstTzInfo 'America/Sao_Paulo' LMT-1 day, 20:54:00 STD\u0026gt;)  Mesmo passando o time zone do pytz como parâmetro tzinfo, a diferença de seis minutos segue (20:54). Ou seja, também não seria a forma correta.\nNa documentação do localize, há apenas a menção:\n Unfortunately using the tzinfo argument of the standard datetime constructors ‘’does not work’’ with pytz for many timezones.\n Ao salvar no banco de dados o objeto aware criado usando o localize, os dados foram, enfim, salvos de forma correta:\n   date_time_tz_aware iso_format_tz_aware date_time_naive isofomat_naive isofomat_naive     2022-05-27 12:30:00.000 -0300 2022-05-27T12:30:00-03:00 2022-05-27 12:30:00.000 2022-05-27T12:30:00 2022-05-27T12:30:00    ✔️ OK, um problema resolvido. Gracias, @georgersilva!\n❓ Mas ainda fica o mistério das conversões entre o dado acessado pelo DBeaver daquele acessado pelo SQLAlchemy.\nEnquanto estava tentando resolver esse segundo problema, o @dunossauro fez uma live de python sobre datetime. Fui assistir e vi que ele indicou usarmos a definição de time zone usando timedelta. Me pareceu sensato.\nVamos testar, então:\nResolvendo problema de definição de time zone com timedelta from datetime import timezone, timedelta # BR_TIME_ZONE = pytz.timezone(\u0026quot;America/Sao_Paulo\u0026quot;) BR_TIME_ZONE = timezone(timedelta(hours=-3)) datetime(2022, 5, 27, 12, 30, 0, 0, tzinfo=BR_TIME_ZONE) # datetime.datetime(2022, 5, 27, 12, 30, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=75600))) datetime(2022, 5, 27, 12, 30, 0, 0).replace(tzinfo=BR_TIME_ZONE) # datetime.datetime(2022, 5, 27, 12, 30, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=75600)))  Reparem que agora não estamos mais usando uma instância timezone do pytz e por isso não podemos usar o método localize(). Com o timedelta, obtivemos os resultados esperados tanto passando o objeto no parâmetro tzinfo, na criação da instância datetime, como ao usar o método replace.\n   id date_time_tz_aware iso_format_tz_aware date_time_naive isofomat_naive      2022-05-27 12:30:00.000 -0300 2022-05-27T12:30:00-03:00 2022-05-27 12:30:00.000 2022-05-27T12:30:00    Dessa forma também temos os dados persistidos corretamente e ainda nos poupa de usar o pytz. Gracias, @dunossauro!\nO mistério das consultas sendo retornadas em UTC e -0300 Só para refrescar a memória: Ao acessar os dados persistidos no banco de dados usando o DBeaver, os recebia com o time zone -0300, enquanto ao acessar pelo SQLAlchemy, os mesmos dados eram retornados em UTC +00:00.\nDecidi acessar o banco e fazer as consultas apresentadas anteriormente pelo psql e pelo DBeaver para confirmar:\n o fuso horário da instância do banco de dados, e; o fuso horário dos dados persistidos;  Confirmando o fuso horário da instância do banco de dados Executando o mesmo comando no psql e DBeaver para uma mesma instância de banco dados tive diferentes retornos:\npsql -h localhost -U postgres -p 5432 postgres show timezone;  Eis, então, que fica evidente: A ferramenta usada para conexão e consulta ao banco de dados é que foram as responsáveis pelas diferenas observadas no time zone.\nIsso me fez lembrar da documentação do PostgreSQL que já havia lido, mas não tinha dado a devida atenção:\n For timestamp with time zone, the internally stored value is always in UTC (Universal Coordinated Time, traditionally known as Greenwich Mean Time, GMT). An input value that has an explicit time zone specified is converted to UTC using the appropriate offset for that time zone. If no time zone is stated in the input string, then it is assumed to be in the time zone indicated by the system\u0026rsquo;s time zone parameter, and is converted to UTC using the offset for the time zone zone. fonte\n Em tradução livre:\n Para dados com informação de time zone, o valor armazenado estará sempre em UTC (também conhecido como GMT). Um valor de entrada que não tenha time zone declarado explicitamente será convertido a UTC usando o time zone indicado pelo sistema.\n A partir disso, várias constatações:\n Os dados que possuem a informação de time zone, são convertidos a UTC. Os dados sem essa definição é entendido como já estando em UTC, logo não é convertido. O DBeaver identificou o time zone da minha máquina e ao retornar uma consulta já convertia todos os dados considerando o time zone da minha maquina. Não é o SQLAlchemy que define como os dados serão resgatados, mas o PostgreSQL. Na verdade, essa definição é feita pela seção de conexão com o banco de dados.  Vejam:\npsql -h localhost -U postgres -p 5432 postgres PostgreSQL= show time zone; # time zone # ---------- # Etc/UTC # (1 row) select * from datetime;  Com uma seção (conexão) recém iniciada, o time zone é configurado para UTC (padrão), com os dados sendo retornados em UTC.\n   id date_time_tz_aware isoformat_tz_aware datetime_naive isoformat_naive     1 2022-05-27 15:36:00+00 2022-05-27T12:30:00-03:06 2022-05-27 15:36:00 2022-05-27T12:30:00-03:06   2 2022-05-27 12:30:00+00 2022-05-27T12:30:00 2022-05-27 12:30:00 2022-05-27T12:30:00    Se, na mesma conexão, eu configuro o time zone para America/Sao_Paulo, e executo a mesma query, os dados na coluna com time zone consciente serão apresentados convertidos ao time zone definido na conexão (-0300).\npostgres=set timezone = 'America/Sao_Paulo'; #SET postgres=show timezone; # timezone # ------------------- # America/Sao_Paulo # (1 row) select * from datetime;  Eis que todos os dados são retornados em -0300:\n   id date_time_tz_aware isoformat_tz_aware datetime_naive isoformat_naive     1 2022-05-27 12:36:00-03 2022-05-27T12:30:00-03:06 2022-05-27 12:30:00 2022-05-27T12:30:00   2 2022-05-27 09:30:00-03 2022-05-27T12:30:00 2022-05-27 12:30:00 2022-05-27T12:30:00    Tudo parece bem óbvio, não? Mas uma coisa que foi fundamental para a minha confusão mental sobre esse comportamento: o fato de estar usando o DBeaver os dados eram apresentados já convertidos para a time zone do meu sistema e, com isso, eu acreditava que os mesmos estavam sendo persistidos como tal no banco de dados. Ao acessar os dados pelo SQLAlchemy (que usa uma seção padrão, sem configuração de time zone, logo em UTC) recebia os dados em UTC. Ficando sem entender o que, de fato, estava sendo persistido.\n:heavy_checkmark: Fica o aprendizado: O PostgreSQL irá retornar os dados de data e hora no time zone da seção de conexão, que por padrão é UTC. Caso vc queira receber-los em outro time zone, basta definir usando o SET timezone, ou, se for usando o SQLAlchemy, você poderá fazê-lo usando o parâmetro connect_args:\nengine = create_engine(..., connect_args={\u0026quot;options\u0026quot;: \u0026quot;-c timezone=-3\u0026quot;})`  coluna naive e aware Ainda que tenha tomado um tempo considerável resolução de todas essas dúvidas, não chegou a esgotar a minha paciência. Por isso, fiz mais alguns testes, para tentar entender, de vez, a diferença entre usar ou não coluna com time zone consciente e ingênua no PostgreSQL.\nAinda que já esteja superada a dúvida sobre as diferenças entre DBeaver e SQLAlchemy, seguirei apresentando as consultas usando ambas ferramentas, pois isso nos ajudará a entender as consequências ao usar campo consciente ou ingênuo.\nPrimeiro teste: Inseri em ambos campos de datetime (consciente e ingênuo), um objeto com time zone consciente:\nrecord = DateTimeTable( date_time_tz_aware=aware, isoformat_tz_aware=f\u0026quot;{aware.isoformat()}\u0026quot;, date_time_naive=aware, isoformat_naive=f\u0026quot;{aware.isoformat()}\u0026quot; ) session.add(record) session.commit()  Ao fazê-lo, o PosgreSQL entenderá o time zone do dado e, como dito anteriormente, os persitirá em UTC (logo, acrescentando três horas). Isso tanto para o campo consciente como para o campo ingênuo. A diferença, contudo estará no resgate da informação por uma seção em UTC ou em outro time zone:\nAcessando esse dado pelo DBeaver (seção com time zone configurado em -0300), tenho o valor do campo aware convertido ao time zone da seção (-0300) e indicando o mesmo, ao passo que o valor persistido no campo naive se mantêm em formato UTC e sem a indicação do time zone:\n   id date_time_tz_aware iso_format_tz_aware date_time_naive isofomat_naive     2 2022-05-27 12:30:00.000 -0300 2022-05-27T12:30:00-03:00 2022-05-27 15:30:00.000 2022-05-27T12:30:00-03:00    Já pelo SQLAlchemy a informação persistida no campo time zone consciente é retornada respeitando o time zone da seção (logo, time zone UTC) e no campo ingênuo, não há alteração.\n   id date_time_tz_aware iso_format_tz_aware date_time_naive isofomat_naive     2 2022-05-27 15:30:00+00:00 2022-05-27T12:30:00-03:00 2022-05-27 15:30:00 2022-05-27T12:30:00-03:00    Segundo teste: Ao inserir em ambos campos, um objeto ingênuo, o PostgreSQL entende que os mesmos já estão em UTC. Logo, ao acessá-los pelo DBeaver (seção com time zone -0300), o valor no campo consciente apresenta o desconto de três horas e apresenta a informação de time zone -0300, e na coluna naive, os valores não são alterados.\n# terceiro registro inserindo datetime naive sempre record = DateTimeTable( date_time_tz_aware=naive, isoformat_tz_aware=f\u0026quot;{naive.isoformat()}\u0026quot;, date_time_naive=naive, isoformat_naive=f\u0026quot;{naive.isoformat()}\u0026quot; ) session.add(record) session.commit() session.close()     id date_time_tz_aware iso_format_tz_aware date_time_naive isofomat_naive     2 2022-05-27 09:30:00.000 -0300 2022-05-27T12:30:00 2022-05-27 12:30:00.000 2022-05-27T12:30:00    Acessando so dados pelo SQLAlchemy, tanto a coluna consciente como a ingênua apresentam o mesmo valor. Contudo, no campo consciente, a informação do time zone é existente (+00:00:00).\n   id date_time_tz_aware iso_format_tz_aware date_time_naive isofomat_naive     2 2022-05-27 12:30:00+00:00 2022-05-27T12:30:00 2022-05-27 12:30:00 2022-05-27T12:30:00    Preparando ambiente de desenvolvimento Para isolar e reproduzir os comportamentos apresentados segui os seguintes passos:\n Criação de um ambiente virtual; Activação do ambiente virtual; Atualização do pip; INstalação dos pacotes listados em requirements.txt; Criei uma instância do banco de dados PostgreSQL usando docker; Criei os modelos as tabelas usando SQLAlchemy;  mkdir datetime cd datetime python -m venv .venv source .venv/bin/activate pip intall --upgrade pip pip install -r requirements.txt  Docker com PostgreSQL Para facilitar, criei uma instância Docker com a imagem original do PostgreSQLQL. Caso já o tenha instalado em sua máquina, desconsidere.\ndocker pull PostgreSQL docker run --name teste_datetime -e PostgreSQL_PASSWORD=password -d PostgreSQL # confirmando existencia docker container ps #CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES #c77150c506a8 PostgreSQL \u0026quot;docker-entrypoint.s…\u0026quot; 6 seconds ago Up 5 seconds  Modelo de dados e conexão com SQLAlchemy Crio, em um arquivo models.py, a classe que representará a tabela datetime do banco de dados. Nela teremos os campos date_time_tz_aware, date_time_aive que são, ambos, DateTime(), com o parâmetro *time zone*=True verdadeiro e falso, respectivamente. Os campos isoformat_tz_aware e isoformat_naive serão os campos textuais que persistirão os dados de data e hora em formato isoformat().\n# models.py import json from sqlalchemy import Integer, DateTime, Text from sqlalchemy import create_engine, Column from sqlalchemy.ext.declarative import declarative_base Base = declarative_base() BD_USERNAME = \u0026quot;PostgreSQL\u0026quot; BD_PASSWORD = \u0026quot;password\u0026quot; BD_HOST = \u0026quot;localhost\u0026quot; BD_PORT = \u0026quot;5433\u0026quot; BD_NAME = \u0026quot;PostgreSQL\u0026quot; def db_connect(): return create_engine( f\u0026quot;PostgreSQLql+psycopg2://{BD_USERNAME}:{BD_PASSWORD}@{BD_HOST}:{BD_PORT}/{BD_NAME}\u0026quot; ) def create_table(engine): Base.metadata.create_all(engine) class DateTimeTable(Base): __tablename__ = \u0026quot;datetime\u0026quot; id = Column(Integer, primary_key=True) date_time_tz_aware = Column(DateTime(timezone=True)) isoformat_tz_aware = Column(Text) date_time_naive = Column(\u0026quot;datetime_naive\u0026quot;, DateTime(timezone=False)) isoformat_naive = Column(Text) engine = db_connect() create_table(engine)  Identificando time zone das instâncias de trabalho Para confirmar que estamos reproduzindo as mesmas situações, vamos confirmar o time zone da base de dados.\nDocker PostgreSQL\npsql -h localhost -U PostgreSQL -p 5433 show timezone; # timezone #---------- # Etc/UTC #(1 row)  Ao executar a consulta select now(), ele me dá a data e hora com a info de time zone utc (+00):\nselect now(); # now #------------------------------- # 2022-05-27 15:36:59.903336+00 #(1 row)  E o mesmo com python:\npython\nfrom datetime import datetime datetime.now().astimezone().tzinfo #datetime.timezone(datetime.timedelta(days=-1, seconds=75600), '-03')  Ou seja, o sistema no qual está rodando o python, está com o time zone -03 em relação ao UTC.\n ⚠️ Atenção, dependendo de como estiver configurado seu sistema, esse resultado poderá ser diferente do meu.\n Resumindo   É possível usar tanto o timedelta como TimeZone, do pytz, para definir o tzinfo de uma instância datetime. Contudo, é preciso cuidado com relação ao método usado na atribuição do tzinfo:\n Caso se esteja usando uma instância TimeZone do pytz, é indicado usar o método localize;  BR_TIME_ZONE = pytz.timezone(\u0026quot;America/Sao_Paulo\u0026quot;) date_time_ibject = datetime(2022, 5, 27, 12, 30, 0, 0) BR_TIME_ZONE.localize(date_time_ibject)   Já usando o timedelta, pode-se fazê-lo tanto na criação da instância datetime, quanto usando o método replace do objeto datetime já instanciado;  BR_TIME_ZONE = timezone(timedelta(hours=-3)) date_time_object = datetime(2022, 5, 27, 12, 30, 0, 0, tzinfo=BR_TIME_ZONE) # OU date_time_object = datetime(2022, 5, 27, 12, 30, 0, 0) date_time_object.replace(tzinfo=BR_TIME_ZONE)    Os dados de data e hora são sempre armazenados em UTC no PostgreSQL, independente de estarmos ou não usando campos com time zone conscientes. Logo, ao persistir um dado consciente, o mesmo será convertido e persistido em UTC, mesmo em campos ingênuos. Objetos sem informação de time zone, serão persistido como tais por se entender já estarem em UTC. A diferença em relação a esses tipos de campos se dá pelo fato do primeiro armazenar a informação do time zone e o último, não.\n  Outra diferença entre campo consciente e ingênuo se dá nas consultas: Os campos conscientes, ao serem consultados por uma seção com time zone diferente do padrão (UTC), retornará os dados convertidos ao time zone da seção;\n Para definir o time zone de uma seção, pode-se usar:  # psql set timezone = 'America/Sao_Paulo';  ou\n# SQLAlchemy engine = create_engine(..., connect_args={\u0026quot;options\u0026quot;: \u0026quot;-c timezone=-3\u0026quot;})    Não poderia deixar de agradecer ao @cuducos pelo incentivo em resolver os problemas encontrados e ajuda na revisão do texto. Eu acabei encontrando essas soluções antes de ter tempo de seguir a sugestão dele: \u0026ldquo;tentar identificar o como o SQLAlchemy estava fazendo o insert dos dados e o resgate dos mesmos\u0026rdquo;. Vejo que, de alguma forma, foi o direcionamento que acabei tomando para entender a diferença nos fuso horários retornados pelas consultas.\n","date":1657843200,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1657855148,"objectID":"aff97362ed3365c4d3c111bfe8f054a5","permalink":"/pt/post/aprendendo-sobre-datetime-sqlalchemy-e-postgresql-a-partir-de-bugs/","publishdate":"2022-07-15T00:00:00Z","relpermalink":"/pt/post/aprendendo-sobre-datetime-sqlalchemy-e-postgresql-a-partir-de-bugs/","section":"post","summary":"Há algum tempo comecei a perceber um \u0026ldquo;comportamento estranho\u0026rdquo; (ainda que tenha colocado o termo bug no título, acho que não é o caso. Foi para atrir mais atenção, mesmo:) relacionado aos dados de data e hora num sistema que estava desenvolvendo.","tags":["Python","pt-br"],"title":"Aprendendo sobre `datetime`, SQLAlchemy e PostgreSQL a partir de bugs","type":"post"},{"authors":[],"categories":["Python"],"content":"Caso não tenha visto as publicações anteriores, deixo aqui o link e os temas abordados:\n Na primeira publicação falo sobre o django-geojson para simular um campo geográfico no models; o geojson para criar um objeto da classe geojson e realizar as validações necessárias para garantir robustez do sistema, e a criação do formulário de registro de dados usando o ModelForm; Na segunda publicação apresento os validadores de campo do Django como uma ferramenta fundamental na qualidade dos dados espaciais, sem depender de infraestrutura SIG (GIS).  Agora a ideia é implementar um webmap usando o módulo django-leaflet para apresentar os fenômenos mapeados com algumas informações no popup do mapa. Para isso iremos:\n usar o GeoJSONLayerView, do django-geojson para retornar os dados salvos no formato apropriado para exibição no webmap; usar o django-leaflet para, além de implementar o webmap, podermos usar várias outras ferramentas (widget);  Vamos lá!\nView GeoJSONLayerView A serialização ou, em inglês serialization, é o processo/mecanismo de tradução dos objetos armazenados na base de dados em outros formatos (em geral, baseado em texto como, por exemplo, XML ou JSON), para serem enviados e/ou consumidos no processo de request/response.\nNo nosso caso isso será importante, pois para apresentar os dados salvos em um webmap, precisaremos servi-los no formato geojson. E é aí que o django-geojson entra. Nós o utilizaremos para fazer a mágica acontecer ao usar a classe GeoJSONLayerView.\nA classe GeoJSONLayerView é um mixin que, em base ao modelo informado do nosso projeto, serializa os dados transformando-os em geojson e os servindo em uma view. Acredite, é bastante coisa para apenas algumas linhas de código.\nPara entender a serialização, segue um exemplo\u0026hellip;\nAo acessar os dados do banco de dados do nosso projeto, temos uma QuerySet.\n\u0026gt;\u0026gt;\u0026gt; Fenomeno.objects.all() \u0026lt;QuerySet [\u0026lt;Fenomeno: fenomeno_teste\u0026gt;]\u0026gt;  Ao acessar a geometria de um objeto do banco de dados do nosso projeto, temos um geojson.\n\u0026gt;\u0026gt;\u0026gt; Fenomeno.objects.get(pk=3).geom {'type': 'Point', 'coordinates': [-42.0, -22.0]}  Ao serializá-lo com o GeoJSONSerializer, temos como retorno uma FeatureCollection seguindo o formato geojson, tendo como propriedades os campos do model:\n\u0026gt;\u0026gt;\u0026gt; from djgeojson.serializers import Serializer as GeoJSONSerializer \u0026gt;\u0026gt;\u0026gt; GeoJSONSerializer().serialize(Fenomeno.objects.all(), use_natural_keys=True, with_modelname=False) '{'crs': {'properties': {'href': 'http://spatialreference.org/ref/epsg/4326/', 'type': 'proj4'}, 'type': 'link'}, 'features': [{'geometry': {'coordinates': [-42.0, -22.0], 'type': 'Point'}, 'id': 3, 'properties': {'data': '2021-06-22', 'hora': '02:07:57', 'nome': 'teste'}, 'type': 'Feature'}], 'type': 'FeatureCollection'}'  Mais sobre serialização pode ser encontrado aqui ou aqui, com outro exemplo relacionado a dado geográfico usando o GeoDjango.\nEntão, ciente de toda a mágica por trás do GeoJSONLayerView e o seu resultado, vamos criar os testes para essa view.\nCriando os testes da view Como estou testando justamente uma view que serializa o objeto do meu modelo em formato geojson, precisarei desses dados salvos no banco de dados. Para tanto, vou adicionar ao setUp do meu TestCase valores válidos ao banco de dados do teste. Sem isso, não poderemos confirmar se a serialização está ocorrendo de forma correta. E, uma vez salvo, realizo um conjunto básico de testes:\n  Confirmo se o status code do request (método \u0026ldquo;get\u0026rdquo;) ao path que pretendo usar para essa views (no caso, \u0026ldquo;/geojson/\u0026quot;), retorna 200, código que indica sucesso no processo de request/response. Veja mais sobre os códigos aqui.\n  Em seguida, confirmo se a resposta recebida é uma FetureCollection com os dados da instância criada anteriormente.\n  # testes.py class FenomenoGeoJsonTest(TestCase): def setUp(self): Fenomeno.objects.create( nome=\u0026quot;Teste\u0026quot;, data=\u0026quot;2020-01-01\u0026quot;, hora=\u0026quot;09:12:12\u0026quot;, geom={\u0026quot;type\u0026quot;: \u0026quot;Point\u0026quot;, \u0026quot;coordinates\u0026quot;: [-42, -22]}, ) def teste_geojson_status_code(self): self.resp = self.client.get(r(\u0026quot;geojson\u0026quot;)) self.assertEqual(200, self.resp.status_code) def teste_path_geojson_returns_valid_feature_collection(self): self.resp = self.client.get(r(\u0026quot;geojson\u0026quot;)) self.assertEqual( self.resp.json(), { \u0026quot;type\u0026quot;: \u0026quot;FeatureCollection\u0026quot;, \u0026quot;features\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;popup_content\u0026quot;: \u0026quot;\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;span\u0026gt;Nome: \u0026lt;/span\u0026gt;Teste\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\u0026quot;, \u0026quot;model\u0026quot;: \u0026quot;core.fenomeno\u0026quot;, }, \u0026quot;id\u0026quot;: 1, \u0026quot;geometry\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;Point\u0026quot;, \u0026quot;coordinates\u0026quot;: [-42.0, -22.0]}, } ], \u0026quot;crs\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;name\u0026quot;, \u0026quot;properties\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;EPSG:4326\u0026quot;}}, }, )  Obviamente, ambos testes falharão, pois, ainda não criamos a view e nem a designamos a um path do nosso sistema.\nPara fazê-los passar, vamos primeiro criar a view: Em views.py criaremos uma classe nova, herdando da classe GeoJSONLayerView. Ela será a view responsável por resgatar os dados e servir-nos como uma FeatureCollection seguindo a estrutura de um geojson.\nUm último detalhe é que, como estamos usando um Class Based-View, ao final a convertemos em view, com o método as_view().\n# views.py from djgeojson.views import GeoJSONLayerView from map_proj.core.models import Fenomeno class FenomenoGeoJson(GeoJSONLayerView): model = Fenomeno properties = (\u0026quot;popup_content\u0026quot;,) fenomeno_geojson = FenomenoGeoJson.as_view()  Adicionando propriedade para popup Percebam que no teste_geojson_FeatureCollection eu já estou considerando que o geojson virá com properties com o nome de popup-content. Essa property ainda deverá ser criada no model em questão e poderá ter quantas informações acharmos pertinentes. Se tratam das informações do model a serem apresentadas no popup do mapa.\nPor agora estou apenas informando o nome do fenômeno mapeado mas, mais à frente, podemos incrementar, adicionando um get_absolute_url por exemplo, para poder acessar aos detalhes do fenômeno diretamente a partir do popup do mapa.\n#models.py ... @property def popup_content(self): return self.nome  Adicionando um path a view Para poder acessar essa view, precisamos incorporá-la na nossa urls.py:\n# urls.py from django.contrib import admin from django.urls import path from map_proj.core.views import fenomeno_geojson # novo! urlpatterns = [ path(\u0026quot;admin/\u0026quot;, admin.site.urls), path(\u0026quot;geojson/\u0026quot;, fenomeno_geojson, name=\u0026quot;geojson\u0026quot;), # novo! ]  Com isso teremos os nossos últimos testes passando. Se ainda assim você tiver curiosidade, pode executar o runserver e acessar os dados pela url http://127.0.0.1:8000/geojson/. O resultado esperado são os dados servidos em geojson:\n{ \u0026quot;type\u0026quot;: \u0026quot;FeatureCollection\u0026quot;, \u0026quot;features\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;popup_content\u0026quot;: \u0026quot;\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;span\u0026gt;Nome: \u0026lt;/span\u0026gt;Teste\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\u0026quot;, \u0026quot;model\u0026quot;: \u0026quot;core.fenomeno\u0026quot;, }, \u0026quot;id\u0026quot;: 1, \u0026quot;geometry\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;Point\u0026quot;, \u0026quot;coordinates\u0026quot;: [-42.0, -22.0]}, } ], \u0026quot;crs\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;name\u0026quot;, \u0026quot;properties\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;EPSG:4326\u0026quot;}}, }  ⚠️ Garanta que você já tenha inserido algum dado ao seu projeto ;)\nPronto, já temos uma view nos servindo os dados em formato geojson. Vamos ao Django-leaflet, para entender como montar um webmap.\nDjango-leaflet Para saber mais sobre o django-leaflet, recomendo dar uma olhada na página pypi e na documentação.\nVocê deve estar se perguntando: \u0026ldquo;por quê usar o django-leaflet se eu posso usar o leaflet \u0026ldquo;puro\u0026rdquo;, já que se trata de uma biblioteca JavaScript para produção do mapa no frontend?\u0026rdquo;.\nOs autores do projeto django-leaflet deixam alguns pontos que justificam sua adoção na página da documentação. Das quais eu destaco:\n Possibilidade de uso das ferramentas de edição de geometría usando os widget; Fácil integração dos widgets na página admin do Django; Controle da aparência dos mapas a partir do Django settings.py;  ⚠️ E por último, mas não menos importante:\n django-leaflet é compatível com os campos django-geojson, o que permite o uso de dados geográficos sem a necessidade de uma base de dados espaciais. O motivo de toda essa série que tenho produzido :)\n Bem legal! Eles criaram um pacote já compatível com o pacote django-geojson, que nos permite simular campos geográficos sem a necessidade de toda a infraestrutura de uma base de dados de SIG (PostGIS, por exemplo).\n⚠️ Porém, atenção ao seguinte detalhe:+\n O django-leaflet depende da biblioteca GDAL, não se esqueça de instalá-la antes.\n Instalando django-leaflet pip install django-leaflet  Após a sua instalação é necessário incluí-lo no settings.py como INSTALLED_APPS.\n⚠️ Não esqueça de adicioná-lo ao requirements.txt do projeto, também.\n# settings.py INSTALLED_APPS = [ ... 'djgeojson', 'leaflet', # novo ... ]  Usando o leaflet Com leaflet instalado, devemos então:\n Na pasta da nossa app, vamos criar uma pasta chamada \u0026ldquo;templates\u0026rdquo;; E nessa pasta, criar um arquivo HTML (neste caso vou chamar de \u0026ldquo;map.html\u0026rdquo;; Nessa página vamos carregar as template_tags do leaflet para poder usar leaflet_js, leaflet_css e o leaflet_map:  Nosso map.html:\n{% load leaflet_tags %} \u0026lt;head\u0026gt; ... {% leaflet_js %} {% leaflet_css %} \u0026lt;/head\u0026gt; ... \u0026lt;body\u0026gt; ... {% leaflet_map \u0026quot;yourmap\u0026quot; %} ... \u0026lt;/body\u0026gt;  Essas template_tags irão tentar acessar as configurações do leaflet presentes no settings.py da app, caso existam. Do contrário, serão usados valores padrão de configuração. O interessante dessas template_tags é que com elas podemos customizar tais configurações a cada template;\nComo a ideia é apenas renderizar essa página, vou adicionar ao urls.py um path a ela, usando o TemplateView. Com isso, ao receber um request neste path, a responsta será direcionada à renderização dessa página:  #urls.py from django.contrib import admin from django.urls import path from django.views.generic import TemplateView from map_proj.core.views import fenomeno_geojson urlpatterns = [ path(\u0026quot;admin/\u0026quot;, admin.site.urls), path(\u0026quot;geojson/\u0026quot;, fenomeno_geojson, name=\u0026quot;geojson\u0026quot;), path(\u0026quot;map/\u0026quot;, TemplateView.as_view(template_name=\u0026quot;map.html\u0026quot;), name=\u0026quot;map\u0026quot;), ]  Isso já o suficiente para termos nosso webmap apresentado:\nImagino que não seja o que esperava, né? Fique calmo. O leaflet buscou as configurações do mapa e, como não encontrou, retornou o mesmo com as configurações padrão. Veremos em breve como alterar as configurações do mapa.\nAntes disso, vamos \u0026ldquo;linkar\u0026rdquo; a view que nos serve o geojson com os dados salvos no banco com o webmap em questão, para que os dados sejam apresentados.\nRenderizando o geojson no mapa Lembra que temos uma view que serializa os dados armazenados no banco e nos serve como uma FeatureCollection e que podemos acessar tais dados pelo path geojson/?\nEntão, iremos adicionar um script à nossa página no qual uma variável dataurl receberá os dados dessa url adicionando tais dados ao mapa, assim que o mesmo for inicializado, desencadeando o processo de construção da popup de cada feição apresentada com sua posterior inserção ao mapa:\n{% load leaflet_tags %} \u0026lt;script\u0026gt; var dataurl = '{% url \u0026quot;geojson\u0026quot; %}'; window.addEventListener(\u0026quot;map:init\u0026quot;, function (event) { var map = event.detail.map; // Download GeoJSON data with Ajax fetch(dataurl) .then(function(resp) { return resp.json(); }) .then(function(data) { L.geoJson(data, { onEachFeature: function onEachFeature(feature, layer) { var props = \u0026quot;\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;span\u0026gt;Nome: \u0026lt;/span\u0026gt; \u0026quot; + feature.properties.popup_content + \u0026quot;\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\u0026quot;; layer.bindPopup(props); }}).addTo(map); }); }); \u0026lt;/script\u0026gt; \u0026lt;head\u0026gt; {% leaflet_js %} {% leaflet_css %} \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {% leaflet_map \u0026quot;yourmap\u0026quot; %} \u0026lt;/body\u0026gt;  Veja que, para a criação da variável dataurl, estamos usando a template_tag do django: var dataurl = '{% url \u0026quot;geojson\u0026quot; %}';\nVeja mais sobre ela aqui.\nRepare também que, neste processo, a cada Feature, será carregada as suas propriedades a serem apresentadas no popup:\nL.geoJson(data, { onEachFeature: function onEachFeature(feature, layer) { var props = \u0026quot;\u0026lt;p\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;span\u0026gt;Nome: \u0026lt;/span\u0026gt; \u0026quot; + feature.properties.popup_content + \u0026quot;\u0026lt;/strong\u0026gt;\u0026lt;/p\u0026gt;\u0026quot;; layer.bindPopup(props); }}).addTo(map);  Com o runserver em execução, já poderemos ver o nosso mapa com o dado carregado e as propriedades que definimos no popup:\n⚠️ Garanta que você já tenha inserido algum dado ao seu projeto ;)\nMudando o tamanho do webmap:\nAntes de passarmos às configurações do leaflet, podemos alterar as dimensões do mapa definindo um style. Por exemplo, para que o mapa ocupe toda a área possível do navegador, basta adicionarmos:\n\u0026lt;style\u0026gt; #yourmap { width: 100%; height: 100%; } \u0026lt;/style\u0026gt;  Configurações do leaflet Bom, além das template_tags do leaflet, o uso do django-leaflet nos permite definirmos as suas configurações no settings.py da app, a partir da seção LEAFLET_CONFIG.\nDentre as configurações possíveis , vou usar apenas o par de coordenadas ao qual o mapa deverá estar centralizado por padrão (DEFAULT_CENTER) e o zoom padrão (DEFAULT_ZOOM):\nLEAFLET_CONFIG = { 'DEFAULT_CENTER': (-22, -42), 'DEFAULT_ZOOM': 7, }  Com isso nosso mapa sempre será apresentado centralizado nas coordenadas (-22, -42) e com o zoom 7:\nPronto: com esses três artigos, já temos um sistema com formulário de inserção de dados, com as devidas validações dos dados preenchidos no mesmo, assim como um webmap apresentando-os ao mundo :-).\nNa próxima publicação vamos ver como fazer o deploy desse sistema no heroku 🚀.\nAté lá!\n","date":1654300800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1654335835,"objectID":"ec0e4d2c176d0aa53e6cffa61b485d87","permalink":"/pt/post/sistema-dados-geograficos-iii/","publishdate":"2022-06-04T00:00:00Z","relpermalink":"/pt/post/sistema-dados-geograficos-iii/","section":"post","summary":"Caso não tenha visto as publicações anteriores, deixo aqui o link e os temas abordados:\n Na primeira publicação falo sobre o django-geojson para simular um campo geográfico no models; o geojson para criar um objeto da classe geojson e realizar as validações necessárias para garantir robustez do sistema, e a criação do formulário de registro de dados usando o ModelForm; Na segunda publicação apresento os validadores de campo do Django como uma ferramenta fundamental na qualidade dos dados espaciais, sem depender de infraestrutura SIG (GIS).","tags":["Python","pt-br"],"title":"Criando um sistema para gestão de dados geográficos de forma simples e robusta III","type":"post"},{"authors":[],"categories":[],"content":"A carreira profissional sob a ótica de um tabuleiro de xadrez  Nota: O presente artigo foi originalmente publicado na primeira edição da revista HBNetwork.\n Aviso antes que seja tarde: não sou formado na área de desenvolvimento, ainda que atue na área de análise de dados e como solucionador de problemas a partir do uso de programação. E tampouco sou um exímio jogador de xadrez. Com relação a este último ponto, como vocês devem se imaginar, estou com as centenas de milhares de pessoas que se permitiram aprender sobre esse lindo jogo depois de assistir à śerie “O gambito da rainha”. Contudo, não considero esses fatos como problema, já que o que pretendo fazer aqui é uma analogia entre o jogo de xadrez e nossa carreira profissional.\nSim, escrever este texto é uma ousadia de minha parte: além de estar fazendo uma analogia entre dois temas aos quais não me considero apto para tal, se trata apenas de um exercício mental e hipotético. Ciente disto, não terei a menor intenção de esgotar todas as possibilidades metafóricas, nem em tê-las como únicas e verdadeiras. A intenção do texto é, nada mais que convidá-los a pensar o nosso desenvolvimento profissional sob uma nova perspectiva, com o único objetivo de exercitar. Logo, não tenho dúvidas que algumas abordagens aqui adotadas deixarão alguns pontos sem explicação/solução.\nAviso dado, comento de onde surgiu a ideia: Sou formado em Geografia e, além de atuar nas áreas mencionadas antes, sou também professor de Geopolítica que, assim como os temas das relações internacionais, possui como metáfora principal o jogo do xadrez. Imagino que já tenham escutado algo como “o xadrez geopolítico no oriente médio”. Mas em que sentido, isso? Devo imaginar o presidente do Irã jogando xadrez contra o presidente do Iraque, onde as peças seriam recursos de seus respectivos países? Ou seria o caso de pensar o mundo como um imenso tabuleiro de xadrez onde cada país desempenha um papel e, neste caso, Irã e Iraque seriam algumas das peças em movimentos importantes? Pois é, ambas possibilidades são pertinentes.\nComo até os menos instruídos no jogo já devem saber, cada peça possui habilidades diferentes de movimentação e captura de outras peças. Reconhecer essas diferentes habilidades é importante para sabermos até que ponto nossos colegas poderão nos dar cobertura em alguma movimentação arriscada ou quando será a nossa vez de fazê-lo para que outros se desenvolvam. Ou podemos, ainda, pensar em cada peça como as diferentes habilidades que temos o potencial de desenvolver. É importante conhecê-las, tentar desenvolvê-las mas sem cair em armadilhas.\nE, só para constar, ainda que tenhamos um oponente ao jogar xadrez, não quero dar muita ênfase a ele nas analogias, para não cair numa visão rasa de que estamos lutando contra alguém, ainda que às vezes seja conveniente: você executa alguns movimentos buscando um objetivo (melhoria salarial, por exemplo) e não o alcança, pois o oponente simplesmente não moveu suas peças como você havia pensado, podendo, então, colocar culpa do seu fracasso no oponente.\nMas vamos ao que interessa: selecionei três conceitos básicos de xadrez a partir dos quais nos permitirão refletir sobre as possíveis abordagens de desenvolvimento pessoal e profissional:\nPeão promovido: Uma iniciativa comum aos que não conhecem alguns conceitos básicos de estratégia é o de tentar a todo custo chegar com o peão ao outro lado do tabuleiro. Cruzando as 8 casas para, então, se tudo der certo, ter o seu peão promovido. Nessa promoção o peão pode se transformar em qualquer uma das outras peças (exceto rei e peão, não faria sentido, né?).\nSeria leviano “apostar todas nossas fichas” em um único peão, peça com movimentação extremamente limitada, que, sozinho, deveria cruzar todo o tabuleiro para então ser promovido a uma peça com maior possibilidade de movimenwtação e que, então, e em teoria, nos daria mais poder. É leviano não apenas pelo risco, mas também pelo fato de dispormos de peças com diferentes habilidades de movimentação.\nJá entenderam onde quero chegar, não? Aliás, a ideia de promoção do peão vem como uma metáfora à ascensão social a partir de um caminho virtuosos (reto).\nReitero: A questão aqui é o custo de oportunidade em sua execução. Além de deixar de desenvolver outras peças com diferentes habilidades, acaba-se por expor uma fraqueza pois, ao não poderem retroceder, o peão poderá ficar como “peão isolado “, outro conceito importante do xadrez. Algo que pode ser como “o vale das sombras” que o Henrique Bastos tanto fala… Ainda que o peão seja considerado uma peça de pouco valor, não vale a pena permitir que ela apresente uma debilidade no desenvolvimento do jogo (carreira).\nImagem: Peão branco na posição e6 está isolado sem proteção de outra peça.\nComo evitar essa debilidade? Existem algumas “estruturas de peões”, onde movem-se diferentes peças dessa categoria de forma que uma dê proteção a outra. Um exemplo está na formação de uma cadeia de peões que, ao estarem em diagonal, acabam por proteger-se de potenciais ataques, não os evitando mas garantindo que ao serem atacados outros poderão contra-atacar. Aliás, uma das defesas mais famosas, chamada de “Defesa Francesa” se baseia nessa proposta. Espírito de equipe e consciência social é tudo, não é mesmo?\nImage: Defesa francesa: grupo de peões negros formando uma cadeia de peões em diagonal, tornando o ataque mais arricado.\nDomínio do centro: Um conceito importante a ser considerado no xadrez é o de domínio do centro do tabuleiro. Longe de definir qualquer partida, o domínio do centro é a estratégia de posicionar algumas peças de forma a ter ao seu alcance o domínio de mais casas (não quero usar a palavra ameaçar pra não entrar numa visão maniqueísta). Mas dito domínio não pode ser pensado apenas pela presença de uma ou outra peça em posição estratégica, como se uma bandeira garantisse, por si só, a posse de um território. É estratégico ter outras peças (habilidades?) dando cobertura às que estão tentando dominar o centro. Assim como se faz com a cadeia de peões. Afinal, ao tentar dominar o centro, se está, por consequência, se expondo, gerando uma debilidade.\nImagem: Peças brancas posicionadas de forma a, não apenas dominar o centro (casas marcadas com círculo verde), mas também, com outras peças protegendo os peões mais avançados (setas verdes).\nPara finalizar, acho legal comentar que, ao meu ver, toda vulnerabilidade mencionada sempre existirá. Afinal, “Só não erra quem não tenta”. E é aí que eu acho que entra um pouco essa ideia de autonomia: reconhecê-las, afinal, ninguém é perfeito, para trabalhá-las de forma a reduzir a perda potencial.\nO que achou dessas análises? Acrescentaria alguma coisa? Não deixe de dar uma olhada nos demais artigos publicados na revista HBNetwork.\nLinks: Portifolio Github Linkedin Twitter\n","date":1651449600,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1651494541,"objectID":"01896e76dd9f61b937ed39a1699a1b0d","permalink":"/pt/post/carreira-e-xadrez/","publishdate":"2022-05-02T00:00:00Z","relpermalink":"/pt/post/carreira-e-xadrez/","section":"post","summary":"A carreira profissional sob a ótica de um tabuleiro de xadrez  Nota: O presente artigo foi originalmente publicado na primeira edição da revista HBNetwork.\n Aviso antes que seja tarde: não sou formado na área de desenvolvimento, ainda que atue na área de análise de dados e como solucionador de problemas a partir do uso de programação.","tags":["Autoconhecimento","Autonomia","pt-br"],"title":"A carreira profissional sob a ótica de um tabuleiro de xadrez","type":"post"},{"authors":[],"categories":["Python"],"content":"Como tudo começou:  Nota do autor: artigo publiciado em 27/04/2022:\n Estou trabalhando num projeto onde uma das funções do python é executada e recebe um um parâmtero pelo terminal. Um detalhe é que esse parâmetro é o nome de uma pessoa. Um ponto que não previ no processo de desenvolvimento é que nomes, como qualquer outro elemento textual da lingua portuguesa, podem ter acentos (ou \u0026ldquo;caracteres especiais\u0026rdquo;).\nPois é, foi praticamente sem querer que vi, olhando os logs produzidos, que os nomes com acento estavam com problema de encoding.\nE assim começou a minha caça ao bug. Uma caça que me tomou um dia e meio. Mas foi de grande aprendizado.\nUm pouco do contexto: Antes de descrever essa aventura, comento um pouco o fluxo do programa que apresentou erro:\n Uma função é executada pelo terminal e recebe um parâmtero, que é o nome de uma pessoa; Esse nome é usado para instanciar um objeto. Logo no __init__ tenho o ponto de acesso ao que foi informado pelo terminal com a incorporação do mesmo como atributo da instância. Alguns processamnetos, que não vem ao caso, são realizados; O log do processamento realizado é persistirdo numa base de dados usando o SQLAlchemy, onde a tabela que o recebe possui um campo id e outro json, com os logs organizados em tal formato.  E agora? Por onde começar? Buscando o bug no terminal\nComo o parâmetro estava sendo passado por terminal, achei que o problema estava nesse ponto: no terminal. Primeiro passo: checar o encoding usado pelo sistema. Mas logo vi que estava tudo em utf-8, o que por sí, não deveria apresentar problema.\nComo o nome estava sendo passado como um parâmetro do sistema a partir de uma variável, aproveitei para checar se o erro não estava aí. Nada que um print e alguns testes no terminal não resolva. E nada, os nomes armazenados na variáve e passados como parâmetro não sofriam qualquer alteração neste processo inicial.\nInterseção terminal/python\nAchei, então que o erro estava em alguma incompatibilidade entre o que era passado no terminal e o como o python estava recebendo.\nSegundo passo, então, foi checar o ponto de contato entre terminal e o python. Ler o seguinte trecho, de uma resposta do stackOverFlow me deu a certeza de que era aí o erro:\n When Python does not detect that it is printing to a terminal, sys.stdout.encoding is set to None.\n Ou seja, quando o python não pode detectar o que está sendo apresentado ao terminal o sys.stdout.encoding é definido como None; Ora, estou passando um parametro a partir de uma variável do terminal, logo um string. O Python não consegue identificar o encoding dessa string e está definindo, então o encoding a None, o que deve estar gerando o erro.\nTentando resolver isso, busquei alguma forma de declarar o encoding\u0026hellip; Cheguei a adicionar ao __init__, quando a classe é instanciada e recebe o nome da pessoa os métodos, .encode().decode('utf-8') ao objeto que recebe o valor. Parecia que ia funcionar, vejam:\nnome = 'Felipe Sodré' b'Felipe Sodr\\xc3\\xa9' # e ao adicionar o decode o texto volta ao normal... nome.encode().decode('utf-8') 'Felipe Sodré'  \u0026ldquo;Meio\u0026rdquo; gambiarra, não? Mas o importante é se funcionar.\nContudo, o que parecia a solução, foi logo por agua abaixo na primeira rodada de teste. O nome continua com erro de encoding.\nDecidí, então, usar o módulo logging para apresentar o nome recebido pelo terminal e nome após a classe estar instanciada durante o processamento. Aliás, o @dunosauro apresentou uma live muito boa sobre o uso do logging\u0026hellip;\nBom, ao usar o logging tive certeza de que estava tentando resolver o erro no ponto errado, todas as mensagens de log estavam sem o tal erro de encoding, mas no log persistido no banco de dados seguia com o maldito erro\u0026hellip;\nSerá que o banco de dados está configurado com uma encoding diferente?\nNo banco de dados\u0026hellip;\n✔️ Banco configurado como \u0026lsquo;utf-8\u0026rsquo;;\nAté que me veio uma luz: nas mensagens de log o nome está sem erro. Mas o log que está sendo persistido no banco de dados ( que são algumas dessas mensagens filtradas para monitorar alguns pontos importantes do sistema) é uma compilação salva em um campo JSON.\n\u0026ldquo;Bom deve ser nesse ponto, então.\u0026rdquo;, pensei.\nReproduzindo o erro em JSON\nParti então para tentar reproduzir esse erro:\n\u0026gt;\u0026gt;\u0026gt;import json \u0026gt;\u0026gt;\u0026gt;info = {'nome':'Felipe Sodré', 'idade':38} \u0026gt;\u0026gt;\u0026gt;info {'nome': 'Felipe Sodré', 'idade': 28} \u0026gt;\u0026gt;\u0026gt;json.dumps(info) '{\u0026quot;nome\u0026quot;: \u0026quot;Felipe Sodr\\xc3\\xa9\u0026quot;, \u0026quot;idade\u0026quot;: 38}'  Pronto! Aí está o problema. No processo de conversão do dicionário ao JSON, há algum tipo de conversão que gera o erro de encoding.\nNão levei \u0026ldquo;muito tempo\u0026rdquo; (tempo é relativo, né?) para encontrar que o método dumps() possui o parâmetro ensure_ascii, com valor padrão True, que garante que as strings do JSON que possuam caracteres não-ASCII estejam com scape.:\n If ensure_ascii is true (the default), the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is false, these characters will be output as-is.\n Testei usando o dumps() com ensur_ascii=False:\n\u0026gt;\u0026gt;\u0026gt;json.dumps(info, ensure_ascii=False) '{\u0026quot;nome\u0026quot;: \u0026quot;Flávia Duarte Nascimento\u0026quot;, \u0026quot;idade\u0026quot;: 12}'  Pronto, ponto de erro encontrado. Basta adicionar o parâmetro apra False e tudo se resolveria.\nMas não foi bem assim, ainda faltava um ponto. Eu não estava gerando o dump e salvando no banco. O que estou fazendo é passar o dado, ainda em dicionário, para o banco usando o SQLAlchemy e ele cuida disso para mim.\nComo, ou melhor, onde, então, eu devo informar esse ensure_ascii?\nEnfim, a solução: Foi lendo essa responta no SOF que entendí que como o ORM SQLAlchemy está cuidadno disso para mim, ele possui um serializador e que o mesmo é, nada mais, nada menos que os métodos jason.dumps() e json.loads(), passados na função create_engine como um kwargs:\nengine = create_engine(..., json_serializer=dumps)\nO golpe final foi ao ler a docuementação do SQLAlchemy sobre o tipo de dado JSON e aprender que podemos customizar o serializador. Olha só o exemplo da documentação, me dando de \u0026ldquo;bandeja\u0026rdquo; a solução para o bug em questão:\nengine = create_engine( \u0026quot;sqlite://\u0026quot;, json_serializer=lambda obj: json.dumps(obj, ensure_ascii=False))  Agora sim, vida que segue, graças à persistẽncia e perseverança na caça aos bugs.\nAh, claro. Essa investigação contou com a ajuda de outros colegas que dedicaram alguns minutos para conversar e propor soluções, tabém. Muito obrigado!\n nota do autor: artigo publiciado em 27/04/2022:\n Quando achei que estava tudo funcionando e coloquei em produção a correção, eis que me deparo com um novo erro. Dessa vez um TypeError:\nraise TypeError(f'Object of type {o.__class__.__name__} ' sqlalchemy.exc.StatementError: (builtins.TypeError) Object of type datetime is not JSON serializable  Com algumas pesquisas, pude identificar que, como estou indicando um serializador, Todo objeto a ser inluido no JSON passará por ele. Mas, como o próprio erro informa, um objeto datetime não pode ser seriaizado. E por isso que o método json.dumps(), além de ter o parâmetro ensure_ascii, possui um argumento para a serialização padrão.\nPortanto o último bug foi resolvido usando o parâmetro default=str. Ou seja, o serializador padrãoa pe transformar o objeto a uma classe string.\nO codigo ficou, então da seguinte forma:\nengine = create_engine( \u0026quot;sqlite://\u0026quot;, json_serializer=lambda obj: json.dumps(obj, ensure_ascii=False, default=str))  E vocês, que estratégias adotam na caça aos bugs?\nNote: image from @ThePracticalDev\n","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1650023953,"objectID":"73d7ef812e4439de59004ac8f8ab4578","permalink":"/pt/post/bug-buster/","publishdate":"2022-04-15T00:00:00Z","relpermalink":"/pt/post/bug-buster/","section":"post","summary":"Como tudo começou:  Nota do autor: artigo publiciado em 27/04/2022:\n Estou trabalhando num projeto onde uma das funções do python é executada e recebe um um parâmtero pelo terminal.","tags":["Python","Autonomia","Bug fixing","pt-br"],"title":"Bug buster","type":"post"},{"authors":[],"categories":null,"content":"Hacktoberfest: Colaborações e aprendizados Artigo publicado também no linkedin.\nO HacktoberFest é um evento promovido pela Digital Ocean durante o mês de outubro e já está na sua oitava edição. O objetivo é incentivar a colaboração em projetos de código aberto e, claro, como uma forma de democratizar o conhecimento em sistemas de versionamento, como o git, além de outras tecnologias.\n \u0026hellip;Ah, e o incentivo vem com a possibilidade de ganhar uma camisa do evento ao ter aprovado quatro pull requests em repositórios participantes (para participar, basta adicionar a tag \u0026ldquo;Hactoberfest\u0026rdquo; ao repositório ou adicionar a tag \u0026ldquo;Hacktoberfest-accepted\u0026rdquo; no Pull request em questão).\n Não foi minha primeira participação, mas foi a primeira vez que pude colaborar em projetos diferentes daqueles relacionados ao meu trabalho cotidiano. E já fazia algum tempo que tinha interesse em colaborar, mas não sabia como quebrar a inércia. Compartilho neste artigo, alguns projetos desenvolvidos este ano e o que pude aprender nos mesmos.\nFogo Cruzado O projeto Fogo Cruzado foi desenvolvido pela Volt Data Lab e Instituto Fogo Cruzado, como um sistema Crowdsourcing para monitoramento dos tiroteios no Rio de Janeiro e/ou em Recife. O mesmo disponibiliza uma API para acessar aos dados, bastando criar um usuário, sem custo. E o projeto já tem um pacote para acessar os dados pelo R.\nComo faltava um módulo python para acessar os dados do projeto, decidi fazê-lo durante o #Hacktoberfest. Esse foi o primeiro projeto: um módulo python para acessar os dados da API, direto do python.\nFoi um desafio legal e, até certa forma, simples, pois eu já tinha um modelo de como funcionava o pacote em R. Então o trabalho foi, principalmente \u0026ldquo;traduzir\u0026rdquo; ao python. Com isso aproveitei para refatorar algumas partes do código.\nNo geral, posso dizer que ao desenvolver esse projeto, aprendi sobre:\n Python-poetry (do zero); Validação de login usando variáveis do sistema; Publicação de módulos no PyPi;  E fica como desafios para melhor/implementar em breve:\n Melhorar o código com type annotation; Criação de documentação com Sphynx (se alguém quiser sugerir outra alternativa será bem-vinda);  PyInaturalist-convert O segundo projeto que atuei nesse mês foi no PyInaturalist-convert. A história deste módulo é bem interessante e surge de uma demanda pessoal: O IMiBio, Instituição onde trabalho, está desenvolvendo um projeto com o INaturalist, uma aplicação de crowdsourcing para observação de biodiversidade, e eu tive que criar um sistema que acesse os dados do projeto usando a API deles. Com isso, conheci o módulo PyINaturalist. Conversando com os desenvolvedores, comentei que seria interessante ter os dados no padrão darwincore. Um deles achou pertinente e começamos a desenvolver juntos. Contudo, fiquei uns bons meses afastado do projeto e ao voltar, já era um pacote bem estruturado. Por isso, para entender a estrutura do mesmo e saber por onde começar, além de ler as issues abertas, adotei a estratégia de ler os testes\u0026hellip;\n\u0026hellip; E foi lendo os testes que percebi que estavam implementando um objeto geojson, \u0026ldquo;na unha\u0026rdquo;. Como estive estudando sobre o geojson (e, inclusive foi um dos temas explorados por mim em outros artigos, propus usá-lo. Com isso, poderíamos usar os métodos de validação já implementados no módulo, garantindo consistência aos dados;\nAo colaborar no módulo PyInaturalist-converter, aprendi sobre:\n Como colaborar a um projeto já estruturado. Tenho certeza que essa não é uma regra. Mas foi uma boa estratégia começar lendo os testes; Mais aprendizados sobre python-poetry :); Soube da existência do formatador de código [Black](The Uncompromising Code Formatter); Soube da existencia do ISORT, para padronizar os imports;  Além desses aprendizados, o autor principal do módulo já tinha configurado no repositório um fluxo de ações e validações bem interessantes. Dessa forma, havia um sistema de validação do que se estava propondo como pull request. Ainda não tive tempo de me aprofundar, mas já está na lista de estudos futuros\u0026hellip;\nAnálise espacial no frontend Uma última atividade que queria compartilhar, não está relacionada a uma contribuição minha, mas sim, um projeto ao qual eu recebi ajuda.\nPubliquei recentemente um artigo apresentando alguns módulos de JavaScript que nos permitem fazer algumas análises espaciais sem depender de uma infraestrutura de servidores de dados e de mapas.\nEu fui apresentado a essas tecnologias no FOSS4G 2021 e por pura curiosidade, já que frontend não é a \u0026ldquo;minha praia\u0026rdquo;, comecei a fazer alguns testes como estratégia de estudos, mesmo.\nPude evoluir bastante com os estudos, mas num momento vi que poderia ser feito muito mais, mas que eu não tinha conhecimento técnico em JS para isso. Não tive dúvidas em contactar um amigo que trabalha com JS e apresentei a ele o que estava tentando fazer. Ele curtiu e acabou colaborando com o projeto, transformando essa prova de conceito numa solução, em algo realmente interessante.\nPercebam que essa colaboração não surgiu pelo Hacktoberfest. Mas por uma mudança de postura minha em me conectar com outras pessoas e apresentar o que eu estava estudando, as \u0026ldquo;minhas dores\u0026rdquo; e o que pretendia fazer.\nNeste projeto estudei e aprendi sobre:\n  O georaster, uma biblioteca JavaScript que nos permite carrregar, e até mesmo criar, dados raster a partir de objetos JavaScript;\n  georaster-layer-for-leaflet que é uma biblioteca que nos permite apresentar dados raster (a princípio geotif) nos mapas feitos em leaflet;\n  geoblaze que é um pacote desenvolvido em JavaScript para permitir analisar dados carregados como georaster.\n  Como resultado, criamos dois visualizadores de dados raster apenas com tecnologia frontend. No primeiro o usuário interage com o pixel (clicando num píxel específico) e o gráfico apresenta o comportamento temporal daquele pixel; No segundo visualizador o usuário clica em um dos estados e o gráfico apresenta o valor médio dos pixels daquele estado ao longo do tempo.\n  Notas finais sobre Hacktoberfest Entendo que muitos \u0026ldquo;torcem o nariz\u0026rdquo; para o Hacktoberfest, pois poucos o utilizam como uma estratégia de estudos, crescimento ou colaboração a projeto de código aberto, que são os objetivos principais. A ideia de escrever sobre as colaborações feitas é justamente destacar que o evento é uma grande estratégia/ferramenta para aprender mais, conectar-se com outros desenvolvedores e se engajar em projetos de código aberto.\nEspero que os meus aprendizados serviam de motivação aos demais. Qualquer coisa, fico à disposição para conversar mais a respeito.\nArtigo publicado também no linkedin.\n","date":1636070400,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1620782712,"objectID":"30dd15b33e4463b0dced345afe53d6b4","permalink":"/pt/post/hacktoberfest-2021/","publishdate":"2021-11-05T00:00:00Z","relpermalink":"/pt/post/hacktoberfest-2021/","section":"post","summary":"Hacktoberfest: Colaborações e aprendizados Artigo publicado também no linkedin.\nO HacktoberFest é um evento promovido pela Digital Ocean durante o mês de outubro e já está na sua oitava edição. O objetivo é incentivar a colaboração em projetos de código aberto e, claro, como uma forma de democratizar o conhecimento em sistemas de versionamento, como o git, além de outras tecnologias.","tags":["Software Livre","Hacktoberfest","Autonomia","pt-br"],"title":"Hacktoberfest 2021","type":"post"},{"authors":[],"categories":null,"content":"Criando um sistema para gestão de dados geográficos de forma simples e robusta Artigo publicado também no linkedin.\nEste ano pude participar do projeto de jornalismo de dados Engolindo Fumaça, desenvolvido pelo InfoAmazonia. Foi um projeto bem desafiador que me trouxe vários aprendizados. Muitos deles já viraram artigos, como os de cubo de dados.\nAinda que o projeto tenha sido um sucesso (inclusive, foi um dos finalistas do prêmio de jornalismo de dados Cláudio Weber Abramo ) alguns desafios ficaram pendentes. Um deles está com a possibilidade de apresentar dados raster em um sistema webmap, sem dispor de grande infraestrutura de SIG, como base de dados e servidor de mapas, PostGIS e geoserver, respectivamente. Afinal, após todo o processo de análise de dados e produção das matérias, era importante apresentar os dados de forma interativa.\nAliás, desenvolvimento de soluções com dados espaciais com infraestrutura limitada tem sido um tema explorado por mim em alguns artigos.\nEntão, em resumo, a necessidade era: apresentar as imagens de satélite utilizadas nas reportagens em um mapa dinâmico, sem depender de um servidor de mapas, para que os leitores da matéria pudessem explorar os dados. Algo similar a um dashboard.\nPois foi ao moderar uma sessão da conferência Free and Open Source Software for Geospatial (#FOSS4G) deste ano que, sem querer, me deparei com as possíveis soluções. A solução seria transportar a responsabilidade de carregar, apresentar e calcular algumas estatísticas ao frontend, usando o conjunto de bibliotecas georaster, georaster-layer-for-leaflet e geoblaze. A apresentação que me dispertou para essas ferramentas foi feita pelo Daniel Dufour sobre o geoblaze.\n georaster é uma biblioteca JavaScript que nos permite carrregar, e até mesmo criar, dados raster a partir de objetos JavaScript; georaster-layer-for-leaflet é uma biblioteca que nos permite apresentar dados raster (a princípio geotif) nos mapas feitos em leaflet; geoblaze é um pacote desenvolvido em JavaScript para permitir analisar dados carregados como georaster.  Dessa forma, com essa stack the bibliotecas poderemos carregar uma imagem raster georreferenciada, extrair estatísticas gerais e espaciais, bem como aplicar alguns processamentos, como algebra de bandas e apresentá-las em um webmap leaflet. Tudo isso sem depender de uma infraestrutura de backend. Tudo sendo processado no frontend. Sim, essa solução pode ser limitada para alguns casos. Mas nem todos.\nE, por isso, decidi explorar essa alternativa, ainda que frontend (e JavaScript) não seja a \u0026ldquo;minha praia\u0026rdquo;. A verdade é que não consegui conter o entusiasmo e parti para uma prova conceitual. Compartilho a prova de conceito que fiz, usando o observablehq (uma espécie de jupyter-notebook para programação frontend).\nAproveitei para consolidar o resultado em uma landingpage. Nela, além de apresentar o raster, foi possível garantir que o usuário possa interagir com o mesmo, de duas formas distintas:\n Clicando em um pixel;   ou clicando em um dos polígonos que representam os limites dos estados que compõem a Amazônia Legal. (⚠️ não estamos representando o estado do Maranhão já que o mesmo não é contemplado integramente na Amazônia Legal);  Em ambas implementações os valores dos pixels são extraídos just-in-time e o gráfico em plotly é atualizado representando o comportamento temporal dos mesmos. Tais dados se referem a Material Particulado \u0026lt; 2.5 do ano de 2020. No gráfico, apresentamos, ainda, o valor máximo sugerido pela Organização Mundial da Saúde (World Health Organization - WHO).\nEssas duas implementações visam explorar oportunidades diferentes das ferramentas em questão. Separamos os resultados em duas páginas diferentes: clicando pixel a pixel; clicando num polígono;\nE, é lógico: tenho tudo documentado no github.\nNão posso deixar de mencionar que o projeto só foi possível com a ajuda do Kyle Felipe quem, inclusive, foi o responsável pela evolução do projeto implementando a solução em JS baseada na seleção de polígonos.\nEspero que seja útil :)\nArtigo publicado também no linkedin.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1635816312,"objectID":"3325d887d2d7333713da2ac53e49c029","permalink":"/pt/post/analise-espacial-frontend/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/pt/post/analise-espacial-frontend/","section":"post","summary":"Criando um sistema para gestão de dados geográficos de forma simples e robusta Artigo publicado também no linkedin.\nEste ano pude participar do projeto de jornalismo de dados Engolindo Fumaça, desenvolvido pelo InfoAmazonia.","tags":["Software Livre","Hacktoberfest","Autonomia","pt-br","JS"],"title":"Análise espacial no frontend","type":"post"},{"authors":[],"categories":[],"content":"Criando um sistema para gestão de dados geográficos de forma simples e robusta II Artigo publicado também no linkedin\nNa primeira publicação onde exploro a possibilidade de implementar um sistema de gestão de dados geoespaciais com Django, sem a necessidade de usar um servidor com PostGIS, vimos sobre:\n o django-geojson para simular um campo geográfico no models; o geojson para criar um objeto da classe geojson e realizar as validações necessárias para garantir robustez do sistema; a criação do formulário de registro de dados usando o ModelForm;  Agora é hora de evoluir e expandir um pouco o sistema criado. Nessa publicação vamos criar validadores de longitude e latitude para poder restringir a inserção de dados a uma determinada região. Com isso, o próximo passo (e artigo) será criar o webmap no nosso sistema. Mas isso fica para breve.\nVamos ao que interessa:\nCriando validadores de longitude e latitude Sobre os validadores: Os validadores (validators, em inglês) fazem parte do sistema de validação de formulários e de campos do Django. Ao criarmos campos de uma determinada classe no nosso modelo, como por exemplo integer, o Django cuidará automaticamente da validação do valor passado a este campo pelo formulário, retornando um erro quando o usuário ingressar um valor de texto no campo em questão, por exemplo. O interessante é que além dos validadores já implementados para cada classe, podemos criar outros, conforme a nossa necessidade.\n Por que necessitamos um validador para os campos de latitude e longitude?\n Como estou explorando o desenvolvimento de um sistema de gestão de dados geográficos com recursos limitados, ou seja, sem uma infraestrutura de operações e consultas espaciais, não poderei consultar se o par de coordenadas inserido pelo usuário está contido nos limites de um determinado estado (uma operação clássica com dados geográficos). Não ter essa possibilidade de validação poderá colocar em risco a qualidade do dado inserido.\nE como não se abre mão quando a questão é qualidade, uma saída será a criação de validadores personalizados para os campos de latitude e longitude, garantindo que esses possuem valores condizentes à nossa área de interesse.\nO que precisamos saber: os validators são funções que recebem um valor, apenas (neste caso, o valor inserido pelo usuário no campo a ser validado), que passará por uma lógica de validação retornando um ValidationError quando o valor inserido não passar na validação. Com o ValidationError podemos customizar uma mensagem de erro, indicando ao usuário o motivo do valor não ter sido considerado válido, para que o mesmo corrija.\nEntão, criarei validadores dos campos de latitude e longitude para sempre que entrarem com valores que não contemplem a área do estado do Rio de Janeiro, um ValidationError será retornado.\n ⚠️ Essa não é uma solução ótima já que, dessa forma, estamos considerando o bounding box do estado em questão, e com isso haverá áreas onde as coordenadas serão válidas, ainda que não estejam internas ao território estadual. Ainda assim, acredito que seja uma solução boa suficiente para alguns casos, principalmente por não depender de toda infraestrutura de GIS.\n O que é um bounding box?\nBounding box poderia ser traduzido por \u0026ldquo;retângulo envolvente\u0026rdquo; do estado, ou de uma feição espacial. Na imagem abaixo, vemos o território do estado do Rio de Janeiro e o retângulo envolvente que limita as suas coordenadas máximas e mínimas de longitude e latitude.\nPercebam que, como mencionado antes, o que conseguimos garantir é que os pares de coordenadas estejam em alguma área interna ao retângulo em questão o que não garante que as mesmas estejam no território do estado do Rio de Janeiro.\nPor uma questão de organização, criei no settings.py do meu projeto as variáveis com os valores máximos e mínimos de latitude e longitude. Essa proposta surgiu do cuducos, e achei que valia a pena implementar. Entendo que é mais organizado e evita possíveis falhas humanas, caso os mesmos valores tenham que ser usados em outras partes do sistema.\nAo fim do meu settings.py, adicionei:\n# settings.py BOUNDING_BOX_LAT_MAX = -20.764962 BOUNDING_BOX_LAT_MIN = -23.366868 BOUNDING_BOX_LON_MAX = -40.95975 BOUNDING_BOX_LON_MIN = -44.887212  Agora, sim. Vamos criar os testes:\n se você não entendeu o motivo pelo qual eu começo criando testes, dá uma olhada na primeira publicação. Nela comento um pouco sobre a abordagem Test Driven Development (TDD).\n Criando os testes: No tests.py, criei uma nova classe de teste TestCase, com o objetivo de testar os validadores simulando o uso do FenomenoForm. Por isso criei staticmethod chamado create_form que cria um dicionário com chaves e valores válidos do formulário em questão, que ao receber um conjunto de argumentos nomeados **kwargs terá tais argumentos atualizados e usados para instanciar e retornar o FenomenoForm.\nFiz isso para, a cada teste, ter uma instância do FenoenoForm alterando apenas os campos que quero simular valores a serem validados, sem ter que passar sempre todos os valores do ModelForm. Assim, eu posso criar diferentes métodos de Test Case, usando o método criado anteriormente alterando o valor inicial a um inválido, testando se de fato um ValidationError é retornado.\n# tests.py class FenomenoFormValidatorsTest(TestCase): @staticmethod def create_form(**kwargs): valid_form = { 'nome': 'Teste', 'data': '2020-01-01', 'hora': '09:12:12', 'longitude': -42, 'latitude': -21} valid_form.update(**kwargs) form = FenomenoForm(valid_form) return form  Nos métodos de teste uso primeiro o assertFalse do método de validação do formulário (form.is_valid()) para confirmar que o mesmo não é valido para, em seguida, testar com o assertEqual se o texto da mensagem de erro é o que esperamos. Veja o link a seguir para saber sobre outros assertions.\n# tests.py def test_max_longitude_raises_error(self): form = self.create_form(longitude='-45') self.assertFalse(form.is_valid()) self.assertEqual(form.errors[\u0026quot;longitude\u0026quot;][0], 'Coordenada longitude fora do contexto do estado do Rio de Janeiro') def test_min_longitude_raises_error(self): form = self.create_form(longitude='-40') self.assertFalse(form.is_valid()) self.assertEqual(form.errors[\u0026quot;longitude\u0026quot;][0], 'Coordenada longitude fora do contexto do estado do Rio de Janeiro') def test_max_latitude_raises_error(self): form = self.create_form(latitude='-24') self.assertFalse(form.is_valid()) self.assertEqual(form.errors[\u0026quot;latitude\u0026quot;][0], 'Coordenada latitude fora do contexto do estado do Rio de Janeiro') def test_min_latitude_raises_error(self): form = self.create_form(latitude='-19') self.assertFalse(form.is_valid()) self.assertEqual(form.errors[\u0026quot;latitude\u0026quot;][0], 'Coordenada latitude fora do contexto do estado do Rio de Janeiro')  Fazemos rodar os testes e teremos erros como esses:\nCreating test database for alias 'default'... System check identified no issues (0 silenced). ...E.E.. ====================================================================== ERROR: test_max_latitude (map_proj.core.tests.FenomenoFormValidatorsTest) ---------------------------------------------------------------------- Traceback (most recent call last): File \u0026quot;/media/felipe/DATA/Repos/Django_Leaflet_Test/map_proj/core/tests.py\u0026quot;, line 78, in test_max_latitude self.assertEqual(form.errors[\u0026quot;latitude\u0026quot;][0], 'Coordenada latitude fora do contexto do estado do Rio de Janeiro') KeyError: 'latitude' ====================================================================== ERROR: test_min_latitude (map_proj.core.tests.FenomenoFormValidatorsTest) ---------------------------------------------------------------------- Traceback (most recent call last): File \u0026quot;/media/felipe/DATA/Repos/Django_Leaflet_Test/map_proj/core/tests.py\u0026quot;, line 83, in test_min_latitude self.assertEqual(form.errors[\u0026quot;latitude\u0026quot;][0], 'Coordenada latitude fora do contexto do estado do Rio de Janeiro') KeyError: 'latitude' ---------------------------------------------------------------------- Ran 8 tests in 0.012s FAILED (errors=2) Destroying test database for alias 'default'...  Ou seja, o forms após ser validado deveria conter um atributo errors tendo como chave o nome do campo que apresentou dados inválidos. Como não temos os validadores criados, nenhum erro de validação foi acusado no campo de latitude.\nCriando e usando validadores: Para superá-los criamos, enfim, os validadores em um arquivo validators.py. Percebam que é nesse ponto que usarei os valores máximos e mínimos de latitude e longitude adicionados no settings.py:\n# validators.py from django.core.exceptions import ValidationError from django.conf import settings def validate_longitude(lon): if lon \u0026lt; settings.BOUNDING_BOX_LON_MIN or lon \u0026gt; settings.BOUNDING_BOX_LON_MAX: raise ValidationError(\u0026quot;Coordenada longitude fora do contexto do estado do Rio de Janeiro\u0026quot;, \u0026quot;erro longitude\u0026quot;) def validate_latitude(lat): if lat \u0026lt; settings.BOUNDING_BOX_LAT_MIN or lat \u0026gt; settings.BOUNDING_BOX_LAT_MAX: raise ValidationError(\u0026quot;Coordenada latitude fora do contexto do estado do Rio de Janeiro\u0026quot;, \u0026quot;erro latitude\u0026quot;)  Com esses validadores estou garantindo que ambos latitude e longitude estejam na área de interesse e, caso contrário, retorno um erro informando ao usuário.\nE é preciso adicioná-los ao forms.py para que sejam usados:\n# forms.py from map_proj.core.validators import validate_longitude, validate_latitude class FenomenoForm(ModelForm): longitude = FloatField(validators=[validate_longitude]) latitude = FloatField(validators=[validate_latitude]) ...  No desenvolvimento dessa solução percebi pelos testes criados que, ao informar uma latitude ou longitude que não passe pela validação, a criação do campo geom se tornava inválido por não receber um desses valores, gerando dois erros: o de validação do campo e o de validação do campo geom. Lembre-se que é no método clean do formulário que o campo geom recebe os valores de longitude e latitude formando uma classe geojson para, logo em seguida ser validado.\nPara evitar isso, alterei o método clean de forma garantir que o campo geom só seja criado e validado, quando ambos valores (longitude e latitude) existirem. Ou seja, tenham passado pelos validadores sem erro.\n#forms.py def clean(self): cleaned_data = super().clean() lon = cleaned_data.get('longitude') lat = cleaned_data.get('latitude') if not all((lon, lat)): raise ValidationError('Erro em latitude ou longitude') cleaned_data['geom'] = Point((lon, lat)) if not cleaned_data['geom'].is_valid: raise ValidationError('Geometria inválida') return cleaned_data   Outro ponto (na verdade, erro) importante que só percebi a partir dos testes é que no forms.py eu não estava considerando o campo geom na lista de fields a serem usados. Com isso o mesmo não é passado ao banco de dados, mesmo passando pelo método clean que o cria.\n Por esse motivo, tive que alterar algumas coisas no forms.py:\n Inseri o campo geom à tupla de fields do forms.py. Inseri o campo geom com um widget de HiddenInput. Esse último, o fiz por se tratar de um campo que não quero expor ao usuário, já que será criado automaticamente no método clean.  Finalmente, a classe Meta do forms.py ficou da seguinte forma:\nclass Meta: model = Fenomeno fields = ('nome', 'data', 'hora', 'latitude', 'longitude', 'geom') widgets = {'geom': HiddenInput()}  Pronto, com tudo isso que fizemos, já temos um sistema que, apesar de não poder fazer consultas espaciais, é capaz de validar os campos de latitude e longitude.\nNo próximo artigo, vou abordar sobre o que está por trás de toda mágica de um webmap, usando o módulo django-leaflet. Enquanto isso, dê uma olhada no que tenho desenvolvido.\nArtigo publicado também no linkedin\n","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1626052857,"objectID":"b2e3e3deb8ab4fce6915c16b9ad8f41f","permalink":"/pt/post/criando-um-sistema-para-gestao-de-dados-geograficos-de-forma-simples-e-robusta-ii/","publishdate":"2021-07-11T00:00:00Z","relpermalink":"/pt/post/criando-um-sistema-para-gestao-de-dados-geograficos-de-forma-simples-e-robusta-ii/","section":"post","summary":"Criando um sistema para gestão de dados geográficos de forma simples e robusta II Artigo publicado também no linkedin\nNa primeira publicação onde exploro a possibilidade de implementar um sistema de gestão de dados geoespaciais com Django, sem a necessidade de usar um servidor com PostGIS, vimos sobre:","tags":["pt-br","Python","Django"],"title":"Criando um sistema para gestão de dados geográficos de forma simples e robusta II","type":"post"},{"authors":[],"categories":["Python"],"content":"Criando um sistema para gestão de dados geográficos de forma simples e robusta Artigo publicado também no linkedin.\nHá algum tempo comecei a estudar sobre desenvolvimento de sistema com Python, usando a framework Django. Decidi expor alguns aprendizados em uma serie de artigos. A ideia é que esses textos me ajudem na consolidação do conhecimento e, ao tê-los publicado, ajudar a outros que tenham interesse na área.\nAproveito para deixar meu agradecimento ao Cuducos que, tanto neste artigo, como em todos meus estudos tem sido um grande mentor. Vamos ao que interessa:\nPor simples, entende-se:\n Um sistema sem a necessidade da instalação e configuração de base de dados PostgreSQL/GIS, Geoserver, etc; Um sistema clássico tipo Create, Retrieve, Update, Delete (CRUD) para dados geográficos; Um sistema que não demande operações e consultas espaciais; Mas um sistema que garanta a qualidade na gestão dos dados geográficos;  Visão geral da proposta: Vamos criar um ambiente virtual Python e instalar a framework Django, para criar o sistema, assim como alguns módulos como jsonfield, que nos vai habilitar a criação de campos JSON em nossa base de dados; django-geojson, que depende do jsonfield e será responsável por habilitar instâncias de dados geográficos, baseando-se em JSON; geojson, que possui todas as regras básicas de validação de dados geográficos, usando a estrutura homônima, geojson.\nO uso desses três módulos nos permitirá o desenvolvimento de um sistema de gestão de dados geográficos sem a necessidade de termos instalado um sistema de gerenciamento de dados geográficos, como o PostGIS. Sim, nosso sistema será bem limitado a algumas tarefas. Mas em contrapartida, poderemos desenvolvê-lo e implementar soluções \u0026ldquo;corriqueiras\u0026rdquo; de forma facilitada.\nNo presente exemplo estarei usando SQLite, como base de dados.\nNosso projeto se chamará de map_proj. E nele vou criar uma app, dentro da pasta do meu projeto Django, chamada core. Essa organização e nomenclatura usada, vem das sugestões do Henrique Bastos. Afinal, o sistema está nascendo. Ainda que eu tenha uma ideia do que ele será, é interessante iniciar com uma aplicação \u0026ldquo;genérica\u0026rdquo; e a partir do momento que o sistema se torne complexo, poderemos desacoplá-la em diferentes aplicações.\nCriando ambiente de desenvolvimento, projeto e nossa app: python -m venv .djleaflet # cria ambiente virtual python # ativando o ambiente virtual: source ./venv/bin/activate # atualizando o pip pip install --upgrade pip # intalando os módulos a serem usados pip install django jsonfield django-geojson geojson # criando projeto django-admin startproject map_proj . # criando app dentro do projeto cd map_proj python manage.py startapp core # criando a base de dados inicial python manage.py migrate # criando superusuário python manage.py createsuperuser  Adicionando os módulos e a app ao projeto Agora é adicionar ao map_proj/settings.py, a app criada e os módulos que usaremos.\n# setting.py INSTALLED_APPS = [ ... 'djgeojson', 'map_proj.core', ]  Perceba que para poder acessar as classes de alto nível criadas pelo pacote djgeojson, teremos que adicioná-lo ao INSTALLED_APPS do settings.py.\nCriando a base de dados Ainda que eu concorde com o Henrique Bastos, que a visão de começar os projetos Django pelo models.py é um tanto \u0026ldquo;perigosa\u0026rdquo;, por colocar ênfase em uma parte da app e, em muitos casos, negligenciar vários outros atributos e ferramentas que o Django nos oferece, irei desconsiderar sua abordagem. Afinal, o objetivo deste artigo não é explorar todo o potencial do Django, mas sim apresentar uma solução simples no desenvolvimento e implementação de um sistema de gestão de dados geográficos para servir como ferramenta de estudo e projeto prático.\nEm models.py usaremos instâncias de alto nível que o Django nos brinda para criar e configurar os campos e as tabelas que teremos em nosso sistema, bem como alguns comportamentos do sistema.\nComo estou desenvolvendo um sistema multi propósito, vou tentar mantê-lo bem genérico. A ideia é que vocês possam imaginar o que adequar para um sistema especialista na sua área de interesse. Vou criar, então, uma tabela para mapear \u0026ldquo;fenômenos\u0026rdquo; (quaisquer). Esse modelo terá os campos nome, data, hora e geometria, a qual será uma instância de PointField.\nO PointField é uma classe criada pelo djgeojson que nos permite usar um campo para dados geográficos sem ter toda a infraestrutura do PostGIS, instalada, por exemplo. Nesse caso, estou simulando um campo de ponto, mas, de acordo com a documentação do pacote, todas as geometrias usadas em dados espaciais são suportadas:\n All geometry types are supported and respectively validated : GeometryField, PointField, MultiPointField, LineStringField, MultiLineStringField, PolygonField, MultiPolygonField, GeometryCollectionField. ( djgeojson )\n # models.py from django.db import models from djgeojson.fields import PointField class Fenomeno(models.Model): nome = models.CharField(max_length=100, verbose_name='Fenomeno mapeado') data = models.DateField(verbose_name='Data da observação') hora = models.TimeField() geom = PointField(blank=True) def __str__(self): return self.nome  Percebam que eu importo de djgeojson a classe PointField. O que o django-geojson fez foi criar uma classe [com estrutura de dados geográfico] de alto nível, mas que no banco de dados será armazenado em um campo JSON. Vale a pena deixar claro: não espero que o usuário do meu sistema saiba preencher o campo geom em formato JSON. Por isso, criarei no forms.py, os campos latitude e longitude e a partir deles, o campo geom será preenchido. Detalharei esse processo mais adiante.\nPronto, já temos o modelo da \u0026lsquo;tabela de dados \u0026ldquo;geográficos\u0026rdquo;\u0026rsquo;, mas esse modelo ainda não foi registrado em nossa base. Para isso:\npython manage.py makemigrations python manage.py migrate  O makemigrations analisa o models.py e o compara com a versão anterior identificando as alterações e criando um arquivo que será executado pelo migrate, aplicando tais alterações ao banco de dados. Aprendi com o Henrique Bastos e Cuducos que o migrate é um sistema de versionamento da estrutura do banco de dados, permitindo retroceder, quando necessário, a outras versões.\nCriando o formulário Vou aproveitar algumas \u0026ldquo;pilhas já incluídas\u0026rdquo; do Django, ao usar o ModelForm para criar o formulário para o carregamento de dados. O ModelForm facilita esse processo.\nAliás, é importante pensar que os formulários do Django vão muito além da \u0026ldquo;carga de dados\u0026rdquo;, já que são os responsáveis por cuidar da interação com o usuário e o(s) processo(s) de validação e limpeza dos dados preenchidos.\nDigo isso, pois ao meu FenomenosForm, eu sobreescrevo o método clean(), que cuida da validação e limpeza do formulário e incluo nele:\n a construção dos dados do campo geom a partir dos valores dos campos de latitude e longitude (criados exclusivamente para a gerção do campo geom); a validação do campo geom;  # forms.py from django.core.exceptions import ValidationError from django.forms import ModelForm, FloatField from map_proj.core.models import Fenomeno from geojson import Point class FenomenoForm(ModelForm): longitude = FloatField() latitude = FloatField() class Meta: model = Fenomeno fields = ('nome', 'data', 'hora', 'latitude', 'longitude') def clean(self): cleaned_data = super().clean() lon = cleaned_data.get('longitude') lat = cleaned_data.get('latitude') cleaned_data['geom'] = Point((lon, lat)) if not cleaned_data['geom'].is_valid: raise ValidationError('Geometria inválida') return cleaned_data  Ainda que pareça simples, não foi fácil chegar a essa estratégia de estruturação dos models e forms. Contei com a ajuda e paciencia do Cuducos. Inicialmente eu mantinha latitude e longitude no meu models. Mas fazendo assim, além de ter uma redundância de dados e uma abertura a erros potenciais, estaria armazenando dados que não devo usar depois de contruir o campo geom. Uma alternativa, discutida com o Cuducos foi de ter tanto latitude como longitude no models, mas o atributo geom como propriedade. Ainda que seja uma estratégia consistente, a redundância se mantém.\nO processo de validação do campo geom também foi fruto de muita discussão. De forma resumida, percebi que o djgeojson apenas valida o tipo de geometria do campo e não a sua consistência. Ao conversar com os desenvolvedores, me disseram que toda a lógica de validação de objetos geojson estavam sendo centralizados no módulo homônimo.\nPor isso eu carrego a classe Point do módulo geojson e designo o campo geom como instância dessa classe. Assim, passo a poder contar com um processo de validação mais consistente, como o método is_valid, usado anteriormente.\nMas e o teste? Pois é, eu adoraria apresentar isso usando a abordagem Test Driven Development (TDD). Mas, talvez pela falta de prática, conhecimento e etc, vou apenas apontar onde e como eu testaria esse sistema. Faço isso como uma forma de estudo, mesmo. Também me pareceu complicado apresentar a abordagem TDD em um artigo, já que a mesma se faz de forma incremental.\nSobre TDD Com o Henrique Bastos e toda a comunidade do Welcome to The Django vi que essa abordagem é tanto filosófica quanto técnica. É praticamente \u0026ldquo;Chora agora, ri depois\u0026rdquo;, mas sem a parte de chorar. Pois com o tempo as coisas ficam mais claras\u0026hellip; Alguns pontos:\n O erro não é para ser evitado no processo de desenvolvimento, mas sim quando estive em produção. Logo, Entenda o que você quer do sistema, crie um teste antes de implementar e deixe o erro te guiar até ter o que deseja; Teste o comportamento esperado e não cada elemento do sistema;  Sem mais delongas:\nO que testar? Vamos usar o arquivo tests.py e criar nossos testes lá. Ao abrir vocês vão ver que já está o comando importando o TestCase.\n Mas o que vamos testar?\n Como pretendo testar tanto a estrutura da minha base de dados, quanto o formulário e, de quebra, a validação do meu campo geom, faço o import do modelo Fenomenos e do form FenomenosForm.\n⚠️ Essa não é uma boa prática. O ideal é criar uma pasta para os testes e separá-los em arquivos distintos. Um para cada elemento do sistema (model, form, view, etc).\nO primeiro teste será a carga de dados. Então, vou instanciar um objeto com o resultado da criação de um elemento do meu model Fenomeno. Faço isso no setUp, para não ter que criá-lo sempre que for fazer um teste relacionado à carga de dados.\nO teste seguinte será relacionado ao formulário e por isso instancio um formulário com os dados carregados e testo a sua validez. Ao fazer isso o formulário passa pelo processo de limpeza, onde está a construção e validação do campo geom. Se qualquer campo for preenchido com dados errados ou inadequados, o django retornará False ao método is_valid. Ou seja, se eu tiver construido o campo geom de forma equivocada, passando mais ou menos parâmetros que o esperado o nosso teste irá avisar, evitando surpresas.\n# tests.py from django.test import TestCase from geojson import Point from map_proj.core.models import Fenomeno from map_proj.core.forms import FenomenoForm class ModelGeomTest(TestCase): def setUp(self): self.fenomeno = Fenomeno.objects.create( nome='Arvore', data='2020-11-06', hora='09:30:00' ) def test_create(self): self.assertTrue(Fenomeno.objects.exists()) class FenomenoFormTest(TestCase): def setUp(self): self.form = FenomenoForm({ 'nome': 'Teste', 'data': '2020-01-01', 'hora': '09:12:12', 'longitude': -45, 'latitude': -22}) self.validation = self.form.is_valid() def test_form_is_valid(self): \u0026quot;\u0026quot;\u0026quot;\u0026quot;form must be valid\u0026quot;\u0026quot;\u0026quot; self.assertTrue(self.validation) def test_geom_coordinates(self): \u0026quot;\u0026quot;\u0026quot;after validating, geom have same values of longitude and latitude\u0026quot;\u0026quot;\u0026quot; self.assertEqual(self.form.cleaned_data['geom'], Point( (self.form.cleaned_data['longitude'], self.form.cleaned_data['latitude']))) def test_geom_is_valid(self): \u0026quot;\u0026quot;\u0026quot;geom must be valid\u0026quot;\u0026quot;\u0026quot; self.assertTrue(self.form.cleaned_data['geom'].is_valid)  ⚠️ Reparem que:\n No test_create() eu testo se existem objetos inseridos no model Fenomeno. Logo, testo se o dado criado no setUp foi corretamente incorporado no banco de dados. Na classe FenomenosFormTest eu crio uma instância do meu modelForm e realizo três testes:  test_form_is_valid() estou testando se os dados carregados são condizentes com o informado no model e, pelo fato desse método usar o método clean(), posso dizer que estou testando indiretamente a validez do campo geom. Caso ele não fosse válido, o form também não seria válido. Em test_geom_coordinates() testo se após a validação o campo geom foi criado como esperado (como uma instância de Point com os dalores de longitude e latitude). O teste test_geom_is_valid() serve para garantir que a contrução do campo geom é valido. Ainda que ao testar se o formulário é valido eu estaria implicitamente testando a validez do campo geom, esse teste serve para garantir a criação válida do campo. Afinal, por algum motivo (como por exemplo, refatoração), pode ser que façamos alguma alteração no método clean() que mantenha o formulário como válido mas deixe de garantir a validez do campo geom.    A diferença entre as classes de teste criadas está no fato de ao inserir os dados usando o método create() - e aconteceria o mesmo se estivesse usando o save() -, apenas será validado se o elemento a ser inserido é condizente com o tipo de coluna no banco de dados. Vale deixar claro: Dessa forma, eu não estou validando a consistência do campo geom, já que o mesmo, caso seja informado, será salvo com sucesso sempre que represente um JSON.\nEsse fato é importante para reforçar o entendimento de que o djgeojson implementa classes de alto nível a serem trabalhados em views e models. No banco, mesmo, temos um campo de JSON. Enquanto que, para poder validar a consistência do campo geom, preciso passar os dados pelo formulário onde, no processo de limpeza do mesmo, o campo será criado e validado usando o módulo geojson. Por isso a classe com os testes relacionados ao comportamento do formulário.\nRegistrando modelo no admin Para facilitar, vou usar o django-admin. Trata-se de uma aplicação já criada onde basta registrar os modelos e views que estamos trabalhando para termos uma interface \u0026ldquo;frontend\u0026rdquo; genérica.\n#admin.py from django.contrib import admin from map_proj.core.models import Fenomeno from map_proj.core.forms import FenomenoForm class FenomenoAdmin(admin.ModelAdmin): model = Fenomeno form = FenomenoForm admin.site.register(Fenomeno, FenomenoAdmin)  To be continued\u0026hellip; Até o momento já temos algo bastante interessante: um sistema de CRUD que nos permite adicionar, editar e remover dados geográficos. Talvez você esteja pensando consigo mesmo:\n \u0026ldquo;OK. Mas o que foi feito até agora, poderia ter sido feito basicamente com uma base de dados que possuam as colunas latitude e longitude\u0026rdquo;.\n Eu diria que sim, até certo ponto. Uma grande diferença, eu diria, da forma como foi implementada é o uso das ferramentas de validação dos dados com o módulo geojson.\nA ideia é, a seguir (e seja lá quando isso for), extender a funcionalidade do sistema ao implementar um webmap para visualizar os dados mapeados.\nArtigo publicado também no linkedin.\n","date":1620345600,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1626053112,"objectID":"a3d4ec61c24e32d9d3526c3a02cefecd","permalink":"/pt/post/criando-um-sistema-para-gestao-de-dados-geograficos-de-forma-simples-e-robusta/","publishdate":"2021-05-07T00:00:00Z","relpermalink":"/pt/post/criando-um-sistema-para-gestao-de-dados-geograficos-de-forma-simples-e-robusta/","section":"post","summary":"Criando um sistema para gestão de dados geográficos de forma simples e robusta Artigo publicado também no linkedin.\nHá algum tempo comecei a estudar sobre desenvolvimento de sistema com Python, usando a framework Django.","tags":["Django","Python","pt-br"],"title":"Criando um sistema para gestão de dados geográficos de forma simples e robusta","type":"post"},{"authors":[],"categories":["R"],"content":" No último artigo fiz uma apresentação breve sobre os arquivos raster armazenados no formato netCDF. No artigo em questão falo apenas dos dados raster, sua estrutura apontando alguns pontos positivos deste formato. Apresentei o pacote ncdf4 e deixei claro a ideia de que\nlibrary(stars) ## Loading required package: abind ## Loading required package: sf ## Linking to GEOS 3.7.1, GDAL 2.2.3, PROJ 4.9.3 (nc.ppm25 \u0026lt;- read_stars(\u0026quot;../2021-03-17-netcdf/rasters/ppm25_2019-06-30.nc\u0026quot;)) ## stars object with 3 dimensions and 1 attribute ## attribute(s): ## Min. 1st Qu. Median Mean ## ppm25_2019-06-30.nc [kg/m^3] 0 3.693478e-09 6.642458e-09 8.645638e-09 ## 3rd Qu. Max. ## ppm25_2019-06-30.nc [kg/m^3] 1.073236e-08 6.336248e-07 ## dimension(s): ## from to offset delta refsys point values x/y ## x 1 75 -74.19 0.4 NA NA NULL [x] ## y 1 59 5.36 -0.4 NA NA NULL [y] ## time 1 6 2019-06-30 12:00:00 UTC 3 hours POSIXct NA NULL library(rasterVis) ## Loading required package: raster ## Loading required package: sp ## Loading required package: lattice ## Loading required package: latticeExtra plot(nc.ppm25) ## downsample set to c(0,0,1) Eu tenho uma variável (ppm\u0026lt;2.5) medida para uma mesma área (cena) ao longo do tempo. O netCDF, como está preparado e pensado para esse tipo de situação já entende os dados como informação de data e hora (!) e os organiza como uma dimensão do dado e não atributo.\nO raster, por ter outros objetivos, carrega tudo como um stack (empilhamento de camadas) e adiciona uma string “X” ao nome das camadas (que são a informação de data e hora), já que as mesmas são numéricas e o pacote em questão restringe o prefixo das camadas a texto, não permitindo valores numéricos. Enfim, nos obriga a digitar mais algumas linhas de código para que o dado fique mais apresentável.\nEssa forma de estruturar os dados fará todo o sentido mais à frente quando formos falar um pouco mais do processamento desses dados.\nO pacote ncdf4 está desenvolvido para o que se convêm chamar de CRUD, na área de desenvolvimento de sistemas: Create, Update and Delete. Ou, seja, a ideia é fornecer ferramentas para manipular e ter controle total dos dados, criando, abrindo, alterando dimensões ou atributos e por aí vai. Mas não para visualização nem para as análises. Essas, ficarão para os próximos artigos.\nEnquanto isso, fique à vontade em me contactar, dar uma alô lá no grupo do GeoCastBrasil no Telegram ou no nosso canal do youtube.\n","date":1619222400,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1616033847,"objectID":"d6e3310a4611aa34a4f50d69bd025130","permalink":"/pt/2021-04-24-netcdf/netcdf/","publishdate":"2021-04-24T00:00:00Z","relpermalink":"/pt/2021-04-24-netcdf/netcdf/","section":"2021-04-24-netcdf","summary":"Entenda o porquê o netCDF é um grande aliado","tags":["pt-br","R"],"title":"Para quem só sabe usar martelo, todo problema é um prego. Até rasters em formato netCDF.","type":"2021-04-24-netcdf"},{"authors":[],"categories":["R"],"content":" Nas últimas semanas estive trabalhando em um projeto com dados atmosféricos. Tem sido de grande aprendizado não apenas por ser uma temática nova, mas também pela demanda de processamento de dados.\nOs aprendizados relacionados ao processamento de dados (sua otimização, claro), serão abordados em artigos ainda em produção. Mas um ponto fundamental e que percebi ser ignorado pela maioria, e neste sentido me incluo, é a estrutura de dados dos arquivos raster em netCDF.\nJá vi, e já respodi a muitas pessoas perguntando como carregar dados netCDF no R, com um simples “com o pacote raster, ora bolas”. No pior das hipóteses, teremos apenas que instalar uma dependencia que é o pacote ncdf4.\nPois é, os dados atmosféricos que tenho trabahado estão em formato netCDF. E usar o pacote raster para carregá-lo e manipulá-lo, me foi útil enquanto a análise era exploratória. Ao ver o volume de dados crescer de três dias para dois meses e, em seguida para dois anos de dados produzidos a cada três horas do dia, todos os dias, totalizando 731 arquivos, percebi que precisaria deixar de preguiça e entender esse tal de netCDF.\nLendo sobre o pacote ncdf41, o formato netCDF é descrito da seguinte forma:\n “binary data files that are portable across platforms and include metadata information in addition to the data sets.”\n Acho nesse fragmento já dá para entender a importância do formato, não?\nVamos entender isso? Vejam a diferença:\nlibrary(raster) (r.ppm25 \u0026lt;- stack(\u0026quot;./rasters/ppm25_2019-06-30.nc\u0026quot;)) ## class : RasterStack ## dimensions : 59, 75, 4425, 6 (nrow, ncol, ncell, nlayers) ## resolution : 0.4, 0.4 (x, y) ## extent : -74.19, -44.19, -18.24, 5.36 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 ## names : X2019.06.30.12.00.00, X2019.06.30.15.00.00, X2019.06.30.18.00.00, X2019.06.30.21.00.00, X2019.07.01.00.00.00, X2019.07.01.03.00.00 Aos que já trabalham com o pacote raster e conhecem os dados raster, poderão ver que, sim, ainda que dependendo do ncdf4, o raster carrega os dados, aparentemente, sem grandes problemas. O mínimo esperado está aí. Um raster, com uma determinada extensão, linhas, colunas, células, bandas ou camadas…\nMas, se os dados estão em netCDF, e esse tipo de dados permite armazenar metadados também, cadê eles?\nPois é. Ao ignorar isso, passava por cima desse detalhe como um marreteiro diante de um parafuso.\nVamos abrir o arquivo com o ncdf4 para começar a identificar alguns pontos importantes, que, por sinal não ficam “apenas” no metadados:\nlibrary(ncdf4) (nc.ppm25 \u0026lt;- nc_open(\u0026quot;./rasters/ppm25_2019-06-30.nc\u0026quot;)) ## File ./rasters/ppm25_2019-06-30.nc (NC_FORMAT_64BIT): ## ## 1 variables (excluding dimension variables): ## short pm2p5[longitude,latitude,time] ## scale_factor: 9.668789576322e-12 ## add_offset: 3.16807559257767e-07 ## _FillValue: -32767 ## missing_value: -32767 ## units: kg m**-3 ## long_name: Particulate matter d \u0026lt; 2.5 um ## standard_name: mass_concentration_of_pm2p5_ambient_aerosol_particles_in_air ## ## 3 dimensions: ## longitude Size:75 ## units: degrees_east ## long_name: longitude ## latitude Size:59 ## units: degrees_north ## long_name: latitude ## time Size:6 *** is unlimited *** ## units: hours since 1900-01-01 00:00:00.0 ## long_name: time ## calendar: gregorian ## ## 2 global attributes: ## Conventions: CF-1.6 ## history: 2021-03-17 20:27:56 GMT by grib_to_netcdf-2.20.0: grib_to_netcdf /data/scratch/20210317-2020/8e/_mars-webmars-public-svc-blue-001-6fe5cac1a363ec1525f54343b6cc9fd8-ZAB6WD.grib -o /data/scratch/20210317-2020/b1/_grib2netcdf-webmars-public-svc-blue-000-6fe5cac1a363ec1525f54343b6cc9fd8-VDR6DW.nc -utime Antes de comentar toda riqueza e detalhamento de informação que nos brinda o ncdf4, percebam que o comando é nc_open(). Não importa de o dado tem uma ou mais “bandas” ou “camadas”. Isso me chamou atenção pois: \u0026gt; como acham que vou saber a quantidade de camadas do dado que quero abrir, se ainda não o conheço?\nMas enfim. Vamos ao mais importante, os detalhes:\nVeja a estruturação. No arquivo em questão, eu tenho uma variável apenas, que possui vários metadados, garantindo minimamente sua compreensão, como: short_name, no caso “pm2p5”, units, para descrever a unidade do dado, long_name e standart_name, além de vários outros.\n 1 variables (excluding dimension variables): short pm2p5[longitude,latitude,time] scale_factor: 9.668789576322e-12 add_offset: 3.16807559257767e-07 _FillValue: -32767 missing_value: -32767 units: kg m**-3 long_name: Particulate matter d \u0026lt; 2.5 um standard_name: mass_concentration_of_pm2p5_ambient_aerosol_particles_in_air E vejam também que são identificadas três dimensões: longitude, latitude e time;\n 3 dimensions: longitude Size:75 units: degrees_east long_name: longitude latitude Size:59 units: degrees_north long_name: latitude time Size:6 *** is unlimited *** units: hours since 1900-01-01 00:00:00.0 long_name: time calendar: gregorian Eis que chegamos ao cubo de dados, não faz sentido?\nFonte da imagem: r-spatial.github.io\n Eu tenho uma variável (ppm\u0026lt;2.5) medida para uma mesma área (cena) ao longo do tempo. O netCDF, como está preparado e pensado para esse tipo de situação já entende os dados como informação de data e hora (!) e os organiza como uma dimensão do dado e não atributo.\nO raster, por ter outros objetivos, carrega tudo como um stack (empilhamento de camadas) e adiciona uma string “X” ao nome das camadas (que são a informação de data e hora), já que as mesmas são numéricas e o pacote em questão restringe o prefixo das camadas a texto, não permitindo valores numéricos. Enfim, nos obriga a digitar mais algumas linhas de código para que o dado fique mais apresentável.\nEssa forma de estruturar os dados fará todo o sentido mais à frente quando formos falar um pouco mais do processamento desses dados.\nO pacote ncdf4 está desenvolvido para o que se convêm chamar de CRUD, na área de desenvolvimento de sistemas: Create, Update and Delete. Ou, seja, a ideia é fornecer ferramentas para manipular e ter controle total dos dados, criando, abrindo, alterando dimensões ou atributos e por aí vai. Mas não para visualização nem para as análises. Essas, ficarão para os próximos artigos.\nEnquanto isso, fique à vontade em me contactar, dar uma alô lá no grupo do GeoCastBrasil no Telegram ou no nosso canal do youtube.\n Não pretendo explorar muitos detalhes, mas o nome do pacote (ncdf4) faz referência à versão 4 de estratura em questão. Portanto, caso você esteja, por algum motivo, usando um arquivo netCDF com versão anterior, o pacote ncdf segue disponível, ainda que descontinuado.↩\n   ","date":1615939200,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1616033847,"objectID":"e49497cd23845e745343c6713cd4ecd6","permalink":"/pt/post/netcdf/","publishdate":"2021-03-17T00:00:00Z","relpermalink":"/pt/post/netcdf/","section":"post","summary":"Entenda o porquê o netCDF é um grande aliado","tags":["pt-br","R"],"title":"Para quem só sabe usar martelo, todo problema é um prego. Até rasters em formato netCDF.","type":"post"},{"authors":null,"categories":null,"content":"","date":1614211200,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1614211200,"objectID":"07aa2289a2d0c3025ef7cf6ac0402ed6","permalink":"/pt/post/o-manifesto-nooscope/","publishdate":"2021-02-25T00:00:00Z","relpermalink":"/pt/post/o-manifesto-nooscope/","section":"post","summary":"Manifesto Nooscope usa alguns conceitos da fotografia, da cartografía como metáfora para destrinchar alguns pontos pouco comentado sobre os algoritmos de Inteligência Artificial - compressões, sintetizações e distorções.","tags":["Inteligência Artificial","pt-br"],"title":"O Manifesto Nooscope","type":"post"},{"authors":null,"categories":null,"content":"","date":1611532800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1611532800,"objectID":"f176c122df3319e489388b50266a64b0","permalink":"/pt/post/cientista-dados-geograficos-como-cheguei-aqui/","publishdate":"2021-01-25T00:00:00Z","relpermalink":"/pt/post/cientista-dados-geograficos-como-cheguei-aqui/","section":"post","summary":"De um processo de reflexão, compartilho com vocês a minha jornada como cientista de dados geográficos.","tags":["Autoconhecimento","Autonomia","pt-br"],"title":"Cientista de dados geográficos - Como cheguei até aqui?!","type":"post"},{"authors":["Felipe Sodré Mendes Barros"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/pt/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/pt/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/pt/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/pt/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1461715200,"objectID":"78cb99c2548e240836a010152c649104","permalink":"/pt/project/domestic-economy/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/pt/project/domestic-economy/","section":"project","summary":"Chatbot de telegram criado para facilitar o registro e acompanhamento de gastos domésticos. Projeto criado como estratégia de estudos em Python e para uso pessoal.","tags":["Chatbot","Pet project","Python"],"title":"Domestic Economy chatbot","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1461715200,"objectID":"d9ab7d10e5bf2eb6e40b77792a53063c","permalink":"/pt/study/scrapy/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/pt/study/scrapy/","section":"study","summary":"Estudo sobre desenvolvimento e implementação de um sistema de raspagem de dados usando Scrapy, SQLAlchemy, logging e Spidermon","tags":["scrapy","sqlalchemy","logging","Python"],"title":"Estudando Scrapy 'like a pro'","type":"study"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1461715200,"objectID":"056aa6241ed7f11298b10a691ea89493","permalink":"/pt/project/restoration-priorization/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/pt/project/restoration-priorization/","section":"project","summary":"Modelo de priorização espacial para restauração florestal, desenvolvido em colaboração com Julia Niemeyer e Renato Crouzeilles.","tags":["Scientific project","R"],"title":"Spatial Restoration Priorization","type":"project"},{"authors":["Felipe Sodré Mendes Barros","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/pt/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/pt/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Felipe Sodré Mendes Barros","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"pt","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/pt/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/pt/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]